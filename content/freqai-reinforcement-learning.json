{"version":2,"kind":"Article","sha256":"2d94e0012de929dd2ecbcd527816ae7f308f614eceaccf0e07cd87545a95711a","slug":"freqai-reinforcement-learning","location":"/freqai-reinforcement-learning.md","dependencies":[],"frontmatter":{"title":"FreqAI 强化学习指南","description":"本文档详细介绍了 FreqAI 的强化学习功能,包括基本概念、运行方法、环境配置等内容。这些功能可以帮助用户构建和训练强化学习模型进行自动交易。","short_title":"强化学习","subtitle":"强化学习模型的详细说明","subject":"FreqAI 强化学习文档","authors":[{"id":"Freqtrade","name":"Freqtrade"}],"github":"https://github.com/freqtrade-cn/docs_zh-CN","keywords":["Freqtrade","中文文档","交易机器人","量化交易","加密货币","数字货币","区块链","人工智能","机器学习"],"numbering":{"title":{"offset":1}},"edit_url":"https://github.com/freqtrade-cn/docs_zh-CN/blob/main/freqai-reinforcement-learning.md","thumbnail":"/tensorboard-27f8850881840b007f5a56a202abb949.jpg","thumbnailOptimized":"/tensorboard-27f8850881840b007f5a56a202abb949.webp","exports":[{"format":"md","filename":"freqai-reinforcement-learning.md","url":"/freqai-reinforcement-cb72db54ba15f96c69ab38f94769520d.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"强化学习","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"i9M8zLGZBr"}],"identifier":"id","label":"强化学习","html_id":"id","implicit":true,"key":"JscNghAkBy"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"安装体积","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"wsxyXUpmKA"}],"key":"MT3M8dPJjF"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"强化学习依赖项包含如 ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"BBbGKLOzjS"},{"type":"inlineCode","value":"torch","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"aSeQA6PN7j"},{"type":"text","value":" 这样的大型包，需在执行 ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"zWvnIfdrK2"},{"type":"inlineCode","value":"./setup.sh -i","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"kdDPQr2jVk"},{"type":"text","value":" 时，在\"Do you also want dependencies for freqai-rl (~700mb additional space required) [y/N]?”问题上选择\"y\"以显式安装。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"hHB1vRnEhH"}],"key":"OxXHigeuVQ"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"喜欢使用 docker 的用户应确保使用带有 ","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"N6ycLku8De"},{"type":"inlineCode","value":"_freqairl","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"xwH3FEWQud"},{"type":"text","value":" 后缀的 docker 镜像。","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"eA8RfAmM7f"}],"key":"Gai7SMZlJD"}],"key":"OSrhSua6Gv"},{"type":"heading","depth":3,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"背景与术语","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"SUrpTGZQcN"}],"identifier":"id","label":"背景与术语","html_id":"id-1","implicit":true,"key":"AtYZrAX0VJ"},{"type":"heading","depth":4,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"什么是 RL，FreqAI 为什么需要它？","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"vRGPoMp3BR"}],"identifier":"id-rl-freqai","label":"什么是 RL，FreqAI 为什么需要它？","html_id":"id-rl-freqai","implicit":true,"key":"tjrS7zTcGi"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"强化学习涉及两个重要组成部分：","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"YXqHNVeTgz"},{"type":"emphasis","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"智能体（agent）","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"KhGruRWjae"},{"type":"emphasis","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"和训练","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"IO2KLMMQtq"}],"key":"gcphDrpfcQ"},{"type":"text","value":"环境（environment）","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"cZEYTz1lNF"}],"key":"tFznmdK3zh"},{"type":"text","value":"。在智能体训练期间，智能体逐根遍历历史蜡烛数据，每次做出一组动作中的一个：多头开仓、多头平仓、空头开仓、空头平仓、中立。在此训练过程中，环境会跟踪这些动作的表现，并根据自定义的 ","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"Ls32RRdOv0"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"RSXyI4l54o"},{"type":"text","value":"（我们为用户提供了一个默认奖励函数，详情见","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"IKWg7vahML"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"QueH159VhI"}],"urlSource":"#creating-a-custom-reward-function","key":"U6mpohfgax"},{"type":"text","value":"）对智能体进行奖励。奖励用于训练神经网络中的权重。","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"DM46uASSNK"}],"key":"XnF1vwzSPH"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"FreqAI RL 实现的另一个重要组成部分是*状态（state）*信息的使用。每一步都会将状态信息（如当前利润、当前持仓、当前交易持续时间）输入网络。这些信息用于训练环境中的智能体，并在 dry/live 时强化智能体（此功能在回测中不可用）。","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"Bz8FfQTGmx"},{"type":"emphasis","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"FreqAI + Freqtrade 是这种强化机制的完美结合，因为这些信息在实时部署中随时可用。","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"rYBIjJ1Yf2"}],"key":"paqLmiObSU"}],"key":"zw9oR6OPUD"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"强化学习是 FreqAI 的自然进化，因为它为市场自适应和反应性增加了新的层次，这是分类器和回归器无法比拟的。然而，分类器和回归器也有 RL 不具备的优势，比如稳健的预测。训练不当的 RL 智能体可能会找到\"漏洞\"或\"技巧\"来最大化奖励，但实际上并未获得任何交易收益。因此，RL 更加复杂，需要比典型分类器和回归器更高的理解水平。","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"OfSJeHVDzl"}],"key":"M6F9gZNLmv"},{"type":"heading","depth":4,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"RL 接口","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"LviAVSP63N"}],"identifier":"rl","label":"RL 接口","html_id":"rl","implicit":true,"key":"vkdEYH41Wh"},{"type":"paragraph","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"在当前框架下，我们旨在通过通用的\"预测模型\"文件暴露训练环境，该文件是用户继承的 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"SgL0RpNIiN"},{"type":"inlineCode","value":"BaseReinforcementLearner","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"j4zd7OOCHu"},{"type":"text","value":" 对象（如 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"NpLHcDckj6"},{"type":"inlineCode","value":"freqai/prediction_models/ReinforcementLearner","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"LvrOw4u00B"},{"type":"text","value":"）。在此用户类中，RL 环境可通过 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"rVK2GnANSq"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"BrloqG1VQK"},{"type":"text","value":" 进行自定义（见","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"KM79dZR5N2"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"下文","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"ZJ4csVld3k"}],"urlSource":"#creating-a-custom-reward-function","key":"ijYlw61wL7"},{"type":"text","value":"）。","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"jtwa9ezy6Y"}],"key":"paTLgYnOAo"},{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"我们设想大多数用户会将精力集中在创造性设计 ","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"S52If62aar"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"cC1fSWQUly"},{"type":"text","value":" 函数（详情见","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"tOfr7gRMd3"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"gnHkDjbw2s"}],"urlSource":"#creating-a-custom-reward-function","key":"L8v1E8SQvv"},{"type":"text","value":"），而对环境的其他部分保持不变。其他用户甚至不会修改环境，只会调整配置和 FreqAI 已有的强大特征工程。与此同时，我们也允许高级用户完全自定义自己的模型类。","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"XkJVbsSb80"}],"key":"I4dxK0XWq7"},{"type":"paragraph","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"该框架基于 stable_baselines3（torch）和 OpenAI gym 构建基础环境类。但总体而言，模型类隔离良好，因此可以轻松集成其他竞争库。环境继承自 ","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"heL0YsYNHP"},{"type":"inlineCode","value":"gym.Env","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"uvv9e7WpBN"},{"type":"text","value":"，因此如需切换到其他库，需编写全新的环境。","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"cmrfWIJdag"}],"key":"xTI7gZ8XPB"},{"type":"heading","depth":4,"position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"重要注意事项","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"vxYHRcX91s"}],"identifier":"id","label":"重要注意事项","html_id":"id-2","implicit":true,"key":"clbf47Q9JD"},{"type":"paragraph","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"如上所述，智能体在一个\"人工\"交易\"环境\"中被\"训练\"。在我们的案例中，这个环境看起来与真实的 Freqtrade 回测环境很相似，但它","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"bgic5uwcRd"},{"type":"emphasis","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"不是","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"C9Kz33F306"}],"key":"KWWXM1toK2"},{"type":"text","value":"。实际上，RL 训练环境要简单得多。它不包含任何复杂的策略逻辑，如 ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"lP6RSMIXfk"},{"type":"inlineCode","value":"custom_exit","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"WSydYV6tnn"},{"type":"text","value":"、","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"U6FsOS1czC"},{"type":"inlineCode","value":"custom_stoploss","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"lvMVxjwN8u"},{"type":"text","value":"、杠杆控制等回调。RL 环境是对真实市场的非常\"原始\"表示，智能体可以自由学习由 ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"ZCXzgboPzk"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"ezRkrA5nQ1"},{"type":"text","value":" 强制执行的策略（如止损、止盈等）。因此，必须注意，智能体训练环境并不等同于真实世界。","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"CXn7wsbeg6"}],"key":"JIZLElnMzT"},{"type":"heading","depth":3,"position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"运行强化学习","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"aDW61NRcfD"}],"identifier":"id","label":"运行强化学习","html_id":"id-3","implicit":true,"key":"tyFX2n9nlO"},{"type":"paragraph","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"text","value":"设置和运行强化学习模型与运行回归器或分类器相同。命令行上必须定义同样的两个参数：","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"iIoXMsKM3b"},{"type":"inlineCode","value":"--freqaimodel","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"hRmmNcZ1wi"},{"type":"text","value":" 和 ","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"hUpLJjvF8m"},{"type":"inlineCode","value":"--strategy","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"Q4TUTgsopu"},{"type":"text","value":"：","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"inkw3ZZtf4"}],"key":"IVekeJUnui"},{"type":"code","lang":"bash","value":"freqtrade trade --freqaimodel ReinforcementLearner --strategy MyRLStrategy --config config.json","position":{"start":{"line":43,"column":1},"end":{"line":45,"column":1}},"key":"qgCTSz6HVF"},{"type":"paragraph","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"其中 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"VoNXeuy5qi"},{"type":"inlineCode","value":"ReinforcementLearner","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"XJkGRlV1qd"},{"type":"text","value":" 将使用 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"NaFi1egdSv"},{"type":"inlineCode","value":"freqai/prediction_models/ReinforcementLearner","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"ec3gIf95A6"},{"type":"text","value":" 中的模板类（或 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"L9UUX3ZsI3"},{"type":"inlineCode","value":"user_data/freqaimodels","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"BJVFVhssRa"},{"type":"text","value":" 下的自定义类）。策略部分则与典型回归器一样，采用","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"xU2dNX0R79"},{"type":"link","url":"/freqai-feature-engineering","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"特征工程","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"KWCSJZjvIu"}],"urlSource":"freqai-feature-engineering.md","dataUrl":"/freqai-feature-engineering.json","internal":true,"protocol":"file","key":"FpDSvxUW8H"},{"type":"text","value":"的 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"AJJNAdjUgJ"},{"type":"inlineCode","value":"feature_engineering_*","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"mxgi23V7PH"},{"type":"text","value":"。不同之处在于目标的创建，强化学习不需要目标标签。然而，FreqAI 要求在动作列中设置一个默认（中立）值：","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"OTEP6Amko3"}],"key":"Qazqp5fYjr"},{"type":"code","lang":"python","value":"    def set_freqai_targets(self, dataframe, **kwargs) -> DataFrame:\n        \"\"\"\n        *仅适用于启用 FreqAI 的策略*\n        设置模型目标的必需函数。\n        所有目标必须以 `&` 开头，以便 FreqAI 内部识别。\n\n        更多特征工程细节见：\n\n        https://www.freqtrade.io/en/latest/freqai-feature-engineering\n\n        :param df: 将接收目标的策略数据框\n        使用示例：dataframe[\"&-target\"] = dataframe[\"close\"].shift(-1) / dataframe[\"close\"]\n        \"\"\"\n        # 对于 RL，无需设置直接目标。此处为占位（中立），直到智能体发送动作。\n        dataframe[\"&-action\"] = 0\n        return dataframe","position":{"start":{"line":49,"column":1},"end":{"line":66,"column":1}},"key":"jkFcrIG18e"},{"type":"paragraph","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"children":[{"type":"text","value":"大部分函数与典型回归器相同，但下方函数展示了策略如何将原始价格数据传递给智能体，以便其在训练环境中访问原始 OHLCV：","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"pjDb4r8uM0"}],"key":"tY7Nd42BqK"},{"type":"code","lang":"python","value":"    def feature_engineering_standard(self, dataframe: DataFrame, **kwargs) -> DataFrame:\n        # 以下特征对 RL 模型是必要的\n        dataframe[f\"%-raw_close\"] = dataframe[\"close\"]\n        dataframe[f\"%-raw_open\"] = dataframe[\"open\"]\n        dataframe[f\"%-raw_high\"] = dataframe[\"high\"]\n        dataframe[f\"%-raw_low\"] = dataframe[\"low\"]\n    return dataframe","position":{"start":{"line":70,"column":1},"end":{"line":78,"column":1}},"key":"iKJD89Q1C5"},{"type":"paragraph","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"children":[{"type":"text","value":"最后，没有显式的\"标签\"需要设置——而是需要分配 ","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"XioqqBSDam"},{"type":"inlineCode","value":"&-action","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"f7YgcnJojr"},{"type":"text","value":" 列，该列在 ","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"WVMxrup4X7"},{"type":"inlineCode","value":"populate_entry/exit_trends()","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"A0sHKEP2Nk"},{"type":"text","value":" 中被访问时包含智能体的动作。在本例中，中立动作为 0。此值应与所用环境一致。FreqAI 提供的两个环境均以 0 作为中立动作。","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"pSNRJDY9QA"}],"key":"TyY339arqi"},{"type":"paragraph","position":{"start":{"line":82,"column":1},"end":{"line":82,"column":1}},"children":[{"type":"text","value":"用户会很快意识到无需设置标签，智能体会\"自主\"做出进出场决策。这使得策略构建变得相当简单。进出场信号由智能体以整数形式给出，直接用于策略中的进出场判断：","position":{"start":{"line":82,"column":1},"end":{"line":82,"column":1}},"key":"vH3Bjf9aCW"}],"key":"JHwvJUnR2Y"},{"type":"code","lang":"python","value":"    def populate_entry_trend(self, df: DataFrame, metadata: dict) -> DataFrame:\n\n        enter_long_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 1]\n\n        if enter_long_conditions:\n            df.loc[\n                reduce(lambda x, y: x & y, enter_long_conditions), [\"enter_long\", \"enter_tag\"]\n            ] = (1, \"long\")\n\n        enter_short_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 3]\n\n        if enter_short_conditions:\n            df.loc[\n                reduce(lambda x, y: x & y, enter_short_conditions), [\"enter_short\", \"enter_tag\"]\n            ] = (1, \"short\")\n\n        return df\n\n    def populate_exit_trend(self, df: DataFrame, metadata: dict) -> DataFrame:\n        exit_long_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 2]\n        if exit_long_conditions:\n            df.loc[reduce(lambda x, y: x & y, exit_long_conditions), \"exit_long\"] = 1\n\n        exit_short_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 4]\n        if exit_short_conditions:\n            df.loc[reduce(lambda x, y: x & y, exit_short_conditions), \"exit_short\"] = 1\n\n        return df","position":{"start":{"line":84,"column":1},"end":{"line":113,"column":1}},"key":"B0qS2E6jI6"},{"type":"paragraph","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"children":[{"type":"text","value":"需要注意的是，","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"VMNKY0WmYs"},{"type":"inlineCode","value":"&-action","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"SIeOWGeb9q"},{"type":"text","value":" 取决于所选环境。上述示例展示了 5 个动作，其中 0 为中立，1 为多头开仓，2 为多头平仓，3 为空头开仓，4 为空头平仓。","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"GdPeJJXVBW"}],"key":"YWvQxHuzjz"},{"type":"heading","depth":3,"position":{"start":{"line":117,"column":1},"end":{"line":117,"column":1}},"children":[{"type":"text","value":"配置强化学习器","position":{"start":{"line":117,"column":1},"end":{"line":117,"column":1}},"key":"TzVJhMbU0k"}],"identifier":"id","label":"配置强化学习器","html_id":"id-4","implicit":true,"key":"HIvQfs1AYE"},{"type":"paragraph","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"children":[{"type":"text","value":"要配置 ","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"mOJppfJd8t"},{"type":"inlineCode","value":"Reinforcement Learner","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"sqR52LKKm7"},{"type":"text","value":"，需在 ","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"b4vcHf5hLw"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"Yi1ys7sQNA"},{"type":"text","value":" 配置中包含如下字典：","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"Y7rL6hegG3"}],"key":"OYjP7HW2Mx"},{"type":"code","lang":"json","value":"        \"rl_config\": {\n            \"train_cycles\": 25,\n            \"add_state_info\": true,\n            \"max_trade_duration_candles\": 300,\n            \"max_training_drawdown_pct\": 0.02,\n            \"cpu_count\": 8,\n            \"model_type\": \"PPO\",\n            \"policy_type\": \"MlpPolicy\",\n            \"model_reward_parameters\": {\n                \"rr\": 1,\n                \"profit_aim\": 0.025\n            }\n        }","position":{"start":{"line":121,"column":1},"end":{"line":135,"column":1}},"key":"Ga1IK440p6"},{"type":"paragraph","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"参数详情见","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"Xvyo2rO1MP"},{"type":"link","url":"/freqai-parameter-table","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"参数表","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"DrDuqtHgmd"}],"urlSource":"freqai-parameter-table.md","dataUrl":"/freqai-parameter-table.json","internal":true,"protocol":"file","key":"I67R4JPKN9"},{"type":"text","value":"。一般来说，","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"XRWSpLBk8b"},{"type":"inlineCode","value":"train_cycles","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"vJz95lmBL8"},{"type":"text","value":" 决定智能体在其人工环境中遍历蜡烛数据以训练模型权重的次数。","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"WYKXYyaHce"},{"type":"inlineCode","value":"model_type","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"ecfNEm4bGc"},{"type":"text","value":" 是一个字符串，选择 ","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"Wo72S2Kbfq"},{"type":"link","url":"https://stable-baselines3.readthedocs.io/en/master/","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"stable_baselines","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"HgOLEGKJhn"}],"urlSource":"https://stable-baselines3.readthedocs.io/en/master/","key":"Z4QoAfZ6u9"},{"type":"text","value":"（外部链接）中可用的模型之一。","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"WRBteKQgXa"}],"key":"LzrT0LDNWl"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"LpR7zfobhZ"}],"key":"SnI8KD4kHe"},{"type":"paragraph","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"children":[{"type":"text","value":"如果你想尝试 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"IH65rnYhfK"},{"type":"inlineCode","value":"continual_learning","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"dp4xt9NBdk"},{"type":"text","value":"，应在主 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"yvqk1BgzGY"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"dxUkO4nneV"},{"type":"text","value":" 配置字典中将其设为 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"nfF0yraVeO"},{"type":"inlineCode","value":"true","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"Tot6YJPBX8"},{"type":"text","value":"。这会让强化学习库在每次再训练时，从前一模型的最终状态继续训练新模型，而不是每次都从头训练。","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"Q4u76Aswpe"}],"key":"jQ40kwHWTx"}],"key":"ETxiVOppaJ"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"YmnDTTP6dC"}],"key":"AhOxGxXvpO"},{"type":"paragraph","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"children":[{"type":"text","value":"请记住，通用的 ","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"B59i5rxPYJ"},{"type":"inlineCode","value":"model_training_parameters","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"iEDD6GQK3D"},{"type":"text","value":" 字典应包含特定 ","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"qf2HLn5Iqq"},{"type":"inlineCode","value":"model_type","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"bKbkjr6fnV"},{"type":"text","value":" 的所有模型超参数。例如，","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"oahTGXLp5l"},{"type":"inlineCode","value":"PPO","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"nh62nlRnOJ"},{"type":"text","value":" 参数见","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"FPTPuufYt2"},{"type":"link","url":"https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"Y67EjdISIV"}],"urlSource":"https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html","key":"D6ad4wUpDR"},{"type":"text","value":"。","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"qP26EHAhWA"}],"key":"AxRFWRRc8l"}],"key":"hvvhBstUC6"},{"type":"heading","depth":3,"position":{"start":{"line":147,"column":1},"end":{"line":147,"column":1}},"children":[{"type":"text","value":"创建自定义奖励函数","position":{"start":{"line":147,"column":1},"end":{"line":147,"column":1}},"key":"EH21ct25gu"}],"identifier":"id","label":"创建自定义奖励函数","html_id":"id-5","implicit":true,"key":"XSAeL4ge6y"},{"type":"admonition","kind":"danger","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"不适用于生产环境","position":{"start":{"line":149,"column":1},"end":{"line":149,"column":1}},"key":"UEulQqFlln"}],"key":"efG8CH1n2j"},{"type":"paragraph","position":{"start":{"line":150,"column":1},"end":{"line":150,"column":1}},"children":[{"type":"text","value":"警告！","position":{"start":{"line":150,"column":1},"end":{"line":150,"column":1}},"key":"K30USb4bH9"}],"key":"rOWnO5RYLV"},{"type":"paragraph","position":{"start":{"line":152,"column":1},"end":{"line":152,"column":1}},"children":[{"type":"text","value":"Freqtrade 源码中提供的奖励函数仅用于展示功能，旨在展示/测试尽可能多的环境控制特性，并在小型计算机上快速运行。它是基准，不适用于生产环境。请注意，你需要自己创建 custom_reward() 函数，或使用其他用户在 Freqtrade 源码之外构建的模板。","position":{"start":{"line":152,"column":1},"end":{"line":152,"column":1}},"key":"EcET6i2sOm"}],"key":"uBNke1Y5G0"}],"key":"ee9VYzOkyV"},{"type":"paragraph","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"当你开始修改策略和预测模型时，会很快发现强化学习器与回归器/分类器有一些重要区别。首先，策略不设置目标值（无标签！）。相反，你需要在 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"tQnsvFsNXA"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"jlGRvr3YeS"},{"type":"text","value":" 类中设置 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"V0Yu1lJo7j"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"euntHFdXzU"},{"type":"text","value":" 函数（见下文）。","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"JOBCFzblxV"},{"type":"inlineCode","value":"prediction_models/ReinforcementLearner.py","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"vyZPfIElDC"},{"type":"text","value":" 中提供了一个默认的 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"QGcOnCPohx"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"KsB2yUTHMa"},{"type":"text","value":"，用于演示奖励构建的必要模块，但","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"z9ajvgQvWE"},{"type":"emphasis","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"不适用于生产","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"Kxa5VqjIep"}],"key":"KbaEDEVc9F"},{"type":"text","value":"。用户","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"pzyApfrjkM"},{"type":"emphasis","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"必须","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"tPNA50vUcX"}],"key":"lkuzA5eovm"},{"type":"text","value":"创建自己的自定义强化学习模型类，或使用 Freqtrade 源码之外的预构建模型并保存到 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"ZfCaWyde9g"},{"type":"inlineCode","value":"user_data/freqaimodels","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"oHpK9XN9gs"},{"type":"text","value":"。在 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"WFwqUEAanT"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"udjLunb1mf"},{"type":"text","value":" 中可以表达你对市场的创造性理论。例如，你可以在智能体获利时奖励它，亏损时惩罚它，或奖励其开仓、惩罚其持仓过久。下方展示了这些奖励的计算方式：","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"jRatm3HUbV"}],"key":"b6i3TXmhUP"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"提示","position":{"start":{"line":157,"column":1},"end":{"line":157,"column":1}},"key":"quoLnfVJY7"}],"key":"vRjtKFa4Qq"},{"type":"paragraph","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"children":[{"type":"text","value":"最好的奖励函数是连续可微且缩放良好的。换句话说，对罕见事件施加一次性大负惩罚不是好主意，神经网络无法学会这种函数。更好的做法是对常见事件施加小负惩罚，这有助于智能体更快学习。你还可以通过让奖励/惩罚随某些线性/指数函数按严重程度缩放来提升连续性。例如，随着持仓时间增加，逐步增加惩罚，这比在某一时刻一次性施加大惩罚更好。","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"key":"ZdbXvP4t7g"}],"key":"M1cBEhevtu"}],"key":"Oww1eqPSA0"},{"type":"code","lang":"python","value":"from freqtrade.freqai.prediction_models.ReinforcementLearner import ReinforcementLearner\nfrom freqtrade.freqai.RL.Base5ActionRLEnv import Actions, Base5ActionRLEnv, Positions\n\n\nclass MyCoolRLModel(ReinforcementLearner):\n    \"\"\"\n    用户自定义 RL 预测模型。\n\n    将此文件保存到 `freqtrade/user_data/freqaimodels`\n\n    然后用如下命令调用：\n\n    freqtrade trade --freqaimodel MyCoolRLModel --config config.json --strategy SomeCoolStrat\n\n    用户可重写 `IFreqaiModel` 继承树中的任意函数。对 RL 来说，最重要的是在此重写 `MyRLEnv`（见下文），以定义自定义 `calculate_reward()`，或重写环境的其他部分。\n\n    此类还允许用户重写 IFreqaiModel 树的其他部分。例如，可以重写 `def fit()`、`def train()` 或 `def predict()` 以精细控制这些过程。\n\n    另一个常见重写是 `def data_cleaning_predict()`，用于精细控制数据处理管道。\n    \"\"\"\n    class MyRLEnv(Base5ActionRLEnv):\n        \"\"\"\n        用户自定义环境。此类继承自 BaseEnvironment 和 gym.Env。\n        用户可重写父类的任意函数。以下为自定义 `calculate_reward()` 的示例。\n\n        警告！\n        此函数仅用于展示功能，旨在展示尽可能多的环境控制特性，并在小型计算机上快速运行。它是基准，不适用于生产环境。\n        \"\"\"\n        def calculate_reward(self, action: int) -> float:\n            # 首先，若动作无效则惩罚\n            if not self._is_valid(action):\n                return -2\n            pnl = self.get_unrealized_profit()\n\n            factor = 100\n\n            pair = self.pair.replace(':', '')\n\n            # 可使用 dataframe 中的特征值\n            # 假设策略中已生成移位的 RSI 指标。\n            rsi_now = self.raw_features[f\"%-rsi-period_10_shift-1_{pair}_\"\n                            f\"{self.config['timeframe']}\"]\\\n                            .iloc[self._current_tick]\n\n            # 奖励智能体开仓\n            if (action in (Actions.Long_enter.value, Actions.Short_enter.value)\n                    and self._position == Positions.Neutral):\n                if rsi_now < 40:\n                    factor = 40 / rsi_now\n                else:\n                    factor = 1\n                return 25 * factor\n\n            # 惩罚智能体未开仓\n            if action == Actions.Neutral.value and self._position == Positions.Neutral:\n                return -1\n            max_trade_duration = self.rl_config.get('max_trade_duration_candles', 300)\n            trade_duration = self._current_tick - self._last_trade_tick\n            if trade_duration <= max_trade_duration:\n                factor *= 1.5\n            elif trade_duration > max_trade_duration:\n                factor *= 0.5\n            # 惩罚持仓不动\n            if self._position in (Positions.Short, Positions.Long) and \\\n            action == Actions.Neutral.value:\n                return -1 * trade_duration / max_trade_duration\n            # 多头平仓\n            if action == Actions.Long_exit.value and self._position == Positions.Long:\n                if pnl > self.profit_aim * self.rr:\n                    factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n                return float(pnl * factor)\n            # 空头平仓\n            if action == Actions.Short_exit.value and self._position == Positions.Short:\n                if pnl > self.profit_aim * self.rr:\n                    factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n                return float(pnl * factor)\n            return 0.","position":{"start":{"line":161,"column":1},"end":{"line":239,"column":1}},"key":"ZwGSRkDEok"},{"type":"heading","depth":3,"position":{"start":{"line":241,"column":1},"end":{"line":241,"column":1}},"children":[{"type":"text","value":"使用 Tensorboard","position":{"start":{"line":241,"column":1},"end":{"line":241,"column":1}},"key":"sbGxeuRrV1"}],"identifier":"id-tensorboard","label":"使用 Tensorboard","html_id":"id-tensorboard","implicit":true,"key":"EfqgRG8lOv"},{"type":"paragraph","position":{"start":{"line":243,"column":1},"end":{"line":243,"column":1}},"children":[{"type":"text","value":"强化学习模型受益于训练指标的跟踪。FreqAI 集成了 Tensorboard，允许用户跨所有币种和所有再训练过程跟踪训练和评估表现。可通过以下命令激活 Tensorboard：","position":{"start":{"line":243,"column":1},"end":{"line":243,"column":1}},"key":"ZyGmR3vEnw"}],"key":"EGSygDTPxe"},{"type":"code","lang":"bash","value":"tensorboard --logdir user_data/models/unique-id","position":{"start":{"line":245,"column":1},"end":{"line":247,"column":1}},"key":"lbxQpaxsT4"},{"type":"paragraph","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"children":[{"type":"text","value":"其中 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"zfDAnM3IqV"},{"type":"inlineCode","value":"unique-id","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"E5tXqPuC85"},{"type":"text","value":" 是 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"tOTzYoyYRL"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"JgPZnqbz8p"},{"type":"text","value":" 配置文件中设置的 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"yGFajhlvYd"},{"type":"inlineCode","value":"identifier","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"H73WQaVNHe"},{"type":"text","value":"。该命令需在单独的 shell 中运行，浏览器访问 127.0.0.1:6006（6006 为 Tensorboard 默认端口）查看输出。","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"kAKQNWd6O5"}],"key":"jdfCQ9mT6b"},{"type":"image","url":"/tensorboard-27f8850881840b007f5a56a202abb949.jpg","alt":"tensorboard","position":{"start":{"line":251,"column":1},"end":{"line":251,"column":1}},"key":"sVfJpbsVV8","urlSource":"assets/tensorboard.jpg","urlOptimized":"/tensorboard-27f8850881840b007f5a56a202abb949.webp"},{"type":"heading","depth":3,"position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"children":[{"type":"text","value":"自定义日志","position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"key":"yF7JCO7JbI"}],"identifier":"id","label":"自定义日志","html_id":"id-6","implicit":true,"key":"E6GE2qyK0T"},{"type":"paragraph","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"children":[{"type":"text","value":"FreqAI 还内置了一个名为 ","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"LuGgNEJlDy"},{"type":"inlineCode","value":"self.tensorboard_log","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"GVirqqM0W8"},{"type":"text","value":" 的分集摘要日志器，用于将自定义信息添加到 Tensorboard 日志。默认情况下，该函数在环境内每步调用一次以记录智能体动作。单集内所有步的值在每集结束时汇总报告，然后所有指标重置为 0，为下一个 episode 做准备。","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"nzwthD7HWN"}],"key":"rRgca29MLX"},{"type":"paragraph","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"children":[{"type":"inlineCode","value":"self.tensorboard_log","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"Bm1xMd9q4k"},{"type":"text","value":" 也可在环境内任意位置使用，例如可在 ","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"liXGk41wf3"},{"type":"inlineCode","value":"calculate_reward","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"j6y2rOHoVA"},{"type":"text","value":" 函数中添加，以收集奖励各部分被调用的频率：","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"cQme2qd7xM"}],"key":"V4QyGK0xYM"},{"type":"code","lang":"python","value":"    class MyRLEnv(Base5ActionRLEnv):\n        \"\"\"\n        用户自定义环境。此类继承自 BaseEnvironment 和 gym.Env。\n        用户可重写父类的任意函数。以下为自定义 `calculate_reward()` 的示例。\n        \"\"\"\n        def calculate_reward(self, action: int) -> float:\n            if not self._is_valid(action):\n                self.tensorboard_log(\"invalid\")\n                return -2\n","position":{"start":{"line":259,"column":1},"end":{"line":270,"column":1}},"key":"bYn42o5mWG"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"aCICGfihgq"}],"key":"mFvyvdoXqX"},{"type":"paragraph","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"children":[{"type":"inlineCode","value":"self.tensorboard_log()","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"vNgUXtbjtI"},{"type":"text","value":" 仅用于跟踪计数对象（如事件、动作）。如需记录浮点数，可作为第二参数传入，如 ","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"N4KgBispH3"},{"type":"inlineCode","value":"self.tensorboard_log(\"float_metric1\", 0.23)","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"Lx50G9JxY9"},{"type":"text","value":"。此时指标值不会累加。","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"kKCKTiD0ZO"}],"key":"TbivHYRv2Z"}],"key":"dUsmBc5HAZ"},{"type":"heading","depth":3,"position":{"start":{"line":276,"column":1},"end":{"line":276,"column":1}},"children":[{"type":"text","value":"选择基础环境","position":{"start":{"line":276,"column":1},"end":{"line":276,"column":1}},"key":"bmUwygh86l"}],"identifier":"id","label":"选择基础环境","html_id":"id-7","implicit":true,"key":"VEGVhVz10g"},{"type":"paragraph","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"children":[{"type":"text","value":"FreqAI 提供三种基础环境：","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"gLQo0s0SMt"},{"type":"inlineCode","value":"Base3ActionRLEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"MvWSWSCW3r"},{"type":"text","value":"、","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"jOMwGWZGNN"},{"type":"inlineCode","value":"Base4ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"lKSC52UdVr"},{"type":"text","value":" 和 ","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"R89NQY7EkL"},{"type":"inlineCode","value":"Base5ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"oZ4lT8URen"},{"type":"text","value":"。顾名思义，这些环境分别适用于可选择 3、4、5 种动作的智能体。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"rLzCWxyIlL"},{"type":"inlineCode","value":"Base3ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"CeGGbs98PY"},{"type":"text","value":" 最简单，智能体可选择持有、多头或空头。该环境也可用于仅做多的机器人（自动跟随策略的 ","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"rfZwptZkOj"},{"type":"inlineCode","value":"can_short","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"zz0udmIuhl"},{"type":"text","value":" 标志），其中多头为开仓，空头为平仓。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"s5dJmCor6Z"},{"type":"inlineCode","value":"Base4ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"RpfRYJBBJs"},{"type":"text","value":" 中，智能体可多头开仓、空头开仓、中立持有或平仓。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"gxsg7JVIlJ"},{"type":"inlineCode","value":"Base5ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"Q96jua3Ywo"},{"type":"text","value":" 则与 Base4 类似，但将平仓动作区分为多头平仓和空头平仓。环境选择的主要影响包括：","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"bbTSra8Tkg"}],"key":"PYZujczxXI"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":280,"column":1},"end":{"line":282,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"children":[{"type":"inlineCode","value":"calculate_reward","position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"key":"dbBSRKlA8S"},{"type":"text","value":" 中可用的动作","position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"key":"wIFG5pNU2v"}],"key":"eGKVtXNfIB"},{"type":"listItem","spread":true,"position":{"start":{"line":281,"column":1},"end":{"line":282,"column":1}},"children":[{"type":"text","value":"用户策略中消费的动作","position":{"start":{"line":281,"column":1},"end":{"line":281,"column":1}},"key":"qiVqRMZEJf"}],"key":"gXjwpAv0a8"}],"key":"tJ1khcN9Hi"},{"type":"paragraph","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"children":[{"type":"text","value":"FreqAI 提供的所有环境均继承自动作/持仓无关的 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"jj4RoC3uwq"},{"type":"inlineCode","value":"BaseEnvironment","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"KAa1WMEjRI"},{"type":"text","value":"，该对象包含所有共享逻辑。架构设计易于自定义。最简单的自定义是 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"LbE0qqWLem"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"QDTUccPD5g"},{"type":"text","value":"（详见","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"QEU2W90ihK"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"d49wOi56DX"}],"urlSource":"#creating-a-custom-reward-function","key":"O7ZhnixBoc"},{"type":"text","value":"）。当然，也可进一步扩展环境内的任意函数，只需在预测模型文件的 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"u15zB8bVho"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"tUI9hxoOno"},{"type":"text","value":" 中重写这些函数即可。更高级的自定义建议直接继承 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"njJpLY0rrM"},{"type":"inlineCode","value":"BaseEnvironment","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"xG3qNX34nR"},{"type":"text","value":" 创建全新环境。","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"XsdWc3ysIy"}],"key":"k2BQCTvymF"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"P12wY5GJFX"}],"key":"CrsJxJgknL"},{"type":"paragraph","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"children":[{"type":"text","value":"仅 ","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"NIaX8itn7L"},{"type":"inlineCode","value":"Base3ActionRLEnv","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"Z5wyNeBIRb"},{"type":"text","value":" 支持仅做多训练/交易（将用户策略属性 ","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"mafDRAnYyH"},{"type":"inlineCode","value":"can_short = False","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"nXwALruKrL"},{"type":"text","value":"）。","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"qI3jLmQZVK"}],"key":"VE7cjNn5k1"}],"key":"MkwDfXU4gn"}],"key":"qzQIhJW0LJ"}],"key":"DEuBvq27CL"},"references":{"cite":{"order":[],"data":{}}}}