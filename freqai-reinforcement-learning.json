{"version":2,"kind":"Article","sha256":"2d94e0012de929dd2ecbcd527816ae7f308f614eceaccf0e07cd87545a95711a","slug":"freqai-reinforcement-learning","location":"/freqai-reinforcement-learning.md","dependencies":[],"frontmatter":{"title":"FreqAI 强化学习指南","description":"本文档详细介绍了 FreqAI 的强化学习功能,包括基本概念、运行方法、环境配置等内容。这些功能可以帮助用户构建和训练强化学习模型进行自动交易。","short_title":"强化学习","subtitle":"强化学习模型的详细说明","subject":"FreqAI 强化学习文档","authors":[{"id":"Freqtrade","name":"Freqtrade"}],"github":"https://github.com/freqtrade-cn/docs_zh-CN","keywords":["Freqtrade","中文文档","交易机器人","量化交易","加密货币","数字货币","区块链","人工智能","机器学习"],"numbering":{"title":{"offset":1}},"edit_url":"https://github.com/freqtrade-cn/docs_zh-CN/blob/main/freqai-reinforcement-learning.md","thumbnail":"/build/tensorboard-27f8850881840b007f5a56a202abb949.jpg","thumbnailOptimized":"/build/tensorboard-27f8850881840b007f5a56a202abb949.webp","exports":[{"format":"md","filename":"freqai-reinforcement-learning.md","url":"/build/freqai-reinforcement-cb72db54ba15f96c69ab38f94769520d.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"强化学习","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"nYfn6iXYj1"}],"identifier":"id","label":"强化学习","html_id":"id","implicit":true,"key":"Z4l5w1wkI3"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"安装体积","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"gtYlK0Relg"}],"key":"Ik0AWXEcoR"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"强化学习依赖项包含如 ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"adowZwQgGt"},{"type":"inlineCode","value":"torch","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"SGBJf937su"},{"type":"text","value":" 这样的大型包，需在执行 ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"tJZw9H3k1P"},{"type":"inlineCode","value":"./setup.sh -i","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"JfjIXrihht"},{"type":"text","value":" 时，在\"Do you also want dependencies for freqai-rl (~700mb additional space required) [y/N]?”问题上选择\"y\"以显式安装。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"B7jHrZCl0F"}],"key":"ylw4W52E6q"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"喜欢使用 docker 的用户应确保使用带有 ","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"rBEIDfrfly"},{"type":"inlineCode","value":"_freqairl","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"XfeBnKjaXD"},{"type":"text","value":" 后缀的 docker 镜像。","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"OfN00D51uG"}],"key":"flPICVFgTk"}],"key":"tYNzL8YXCX"},{"type":"heading","depth":3,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"背景与术语","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"g0TO8sb0ls"}],"identifier":"id","label":"背景与术语","html_id":"id-1","implicit":true,"key":"tPvhZRR7m3"},{"type":"heading","depth":4,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"什么是 RL，FreqAI 为什么需要它？","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"WaoI4jlKIO"}],"identifier":"id-rl-freqai","label":"什么是 RL，FreqAI 为什么需要它？","html_id":"id-rl-freqai","implicit":true,"key":"Q0UdqJHkL5"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"强化学习涉及两个重要组成部分：","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"XZlvy3G3YA"},{"type":"emphasis","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"智能体（agent）","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"RvLugzy3Q6"},{"type":"emphasis","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"和训练","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"JdJPyDDIGj"}],"key":"mQ0M9MUUCi"},{"type":"text","value":"环境（environment）","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"DPtO9Hm5ML"}],"key":"fBKmFWAiKa"},{"type":"text","value":"。在智能体训练期间，智能体逐根遍历历史蜡烛数据，每次做出一组动作中的一个：多头开仓、多头平仓、空头开仓、空头平仓、中立。在此训练过程中，环境会跟踪这些动作的表现，并根据自定义的 ","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"rlX05nTyQd"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"ATXGyjCr4B"},{"type":"text","value":"（我们为用户提供了一个默认奖励函数，详情见","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"Qi2vFkAHqP"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"i66eVmKFkl"}],"urlSource":"#creating-a-custom-reward-function","key":"BplxMk9H9C"},{"type":"text","value":"）对智能体进行奖励。奖励用于训练神经网络中的权重。","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"UyXBqccrTs"}],"key":"oi2GXPcMQc"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"FreqAI RL 实现的另一个重要组成部分是*状态（state）*信息的使用。每一步都会将状态信息（如当前利润、当前持仓、当前交易持续时间）输入网络。这些信息用于训练环境中的智能体，并在 dry/live 时强化智能体（此功能在回测中不可用）。","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"fAlWyfHocA"},{"type":"emphasis","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"FreqAI + Freqtrade 是这种强化机制的完美结合，因为这些信息在实时部署中随时可用。","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"LLrFX1bXtk"}],"key":"O4SldV6AkZ"}],"key":"PViFmWIjch"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"强化学习是 FreqAI 的自然进化，因为它为市场自适应和反应性增加了新的层次，这是分类器和回归器无法比拟的。然而，分类器和回归器也有 RL 不具备的优势，比如稳健的预测。训练不当的 RL 智能体可能会找到\"漏洞\"或\"技巧\"来最大化奖励，但实际上并未获得任何交易收益。因此，RL 更加复杂，需要比典型分类器和回归器更高的理解水平。","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"Q2io6y5qer"}],"key":"CIJiiBaZFP"},{"type":"heading","depth":4,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"RL 接口","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"MKLEpHyBnL"}],"identifier":"rl","label":"RL 接口","html_id":"rl","implicit":true,"key":"C2GDMfYWK9"},{"type":"paragraph","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"在当前框架下，我们旨在通过通用的\"预测模型\"文件暴露训练环境，该文件是用户继承的 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"NqFRC8RCkn"},{"type":"inlineCode","value":"BaseReinforcementLearner","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"a6tgBbssIS"},{"type":"text","value":" 对象（如 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"rGFDwbe3kG"},{"type":"inlineCode","value":"freqai/prediction_models/ReinforcementLearner","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"NF1TzTDN9P"},{"type":"text","value":"）。在此用户类中，RL 环境可通过 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"opTcX6OZWK"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"YqdCkRkMwt"},{"type":"text","value":" 进行自定义（见","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"ZXe7PWVFMb"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"下文","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"EapvGQ7fio"}],"urlSource":"#creating-a-custom-reward-function","key":"YcgpnarKlm"},{"type":"text","value":"）。","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"SAVoXEiT3e"}],"key":"lyfHaDOQfQ"},{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"我们设想大多数用户会将精力集中在创造性设计 ","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"OHHPJBlhBs"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"AWS01J86TS"},{"type":"text","value":" 函数（详情见","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"nXDiWHfbpW"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"WvJeaapcV5"}],"urlSource":"#creating-a-custom-reward-function","key":"J8uElG4iSZ"},{"type":"text","value":"），而对环境的其他部分保持不变。其他用户甚至不会修改环境，只会调整配置和 FreqAI 已有的强大特征工程。与此同时，我们也允许高级用户完全自定义自己的模型类。","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"T12N4M8EOG"}],"key":"qZ0foaMeWk"},{"type":"paragraph","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"该框架基于 stable_baselines3（torch）和 OpenAI gym 构建基础环境类。但总体而言，模型类隔离良好，因此可以轻松集成其他竞争库。环境继承自 ","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"SGMyWMGm4s"},{"type":"inlineCode","value":"gym.Env","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"cRn7qnA7rL"},{"type":"text","value":"，因此如需切换到其他库，需编写全新的环境。","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"OOwB0GK4pf"}],"key":"lzWgsbv8MI"},{"type":"heading","depth":4,"position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"重要注意事项","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"uTtpBavPQa"}],"identifier":"id","label":"重要注意事项","html_id":"id-2","implicit":true,"key":"t00YxponvE"},{"type":"paragraph","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"如上所述，智能体在一个\"人工\"交易\"环境\"中被\"训练\"。在我们的案例中，这个环境看起来与真实的 Freqtrade 回测环境很相似，但它","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"RVSyApJSFL"},{"type":"emphasis","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"不是","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"hN4mZ5HaRO"}],"key":"s7ld5IPfRS"},{"type":"text","value":"。实际上，RL 训练环境要简单得多。它不包含任何复杂的策略逻辑，如 ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"JmgircrHc8"},{"type":"inlineCode","value":"custom_exit","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"sGgFe9aByt"},{"type":"text","value":"、","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"AHzZkL3d0E"},{"type":"inlineCode","value":"custom_stoploss","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"OHMwFQv57L"},{"type":"text","value":"、杠杆控制等回调。RL 环境是对真实市场的非常\"原始\"表示，智能体可以自由学习由 ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"pEs9FLgokl"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"uWLjdJqWA9"},{"type":"text","value":" 强制执行的策略（如止损、止盈等）。因此，必须注意，智能体训练环境并不等同于真实世界。","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"GLFahFYkTX"}],"key":"lJZZ9do33j"},{"type":"heading","depth":3,"position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"运行强化学习","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"WzPNyDSvjM"}],"identifier":"id","label":"运行强化学习","html_id":"id-3","implicit":true,"key":"ePED6osX6o"},{"type":"paragraph","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"text","value":"设置和运行强化学习模型与运行回归器或分类器相同。命令行上必须定义同样的两个参数：","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"Gbziwe06KP"},{"type":"inlineCode","value":"--freqaimodel","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"Jb1lchQAYW"},{"type":"text","value":" 和 ","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"oEw1OgTZlR"},{"type":"inlineCode","value":"--strategy","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"PhppRHVAyl"},{"type":"text","value":"：","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"o59uurXT18"}],"key":"vb1NN7Kwj4"},{"type":"code","lang":"bash","value":"freqtrade trade --freqaimodel ReinforcementLearner --strategy MyRLStrategy --config config.json","position":{"start":{"line":43,"column":1},"end":{"line":45,"column":1}},"key":"dkGN4wrHVA"},{"type":"paragraph","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"其中 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"Vb9gOzesnX"},{"type":"inlineCode","value":"ReinforcementLearner","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"kS4JPex86X"},{"type":"text","value":" 将使用 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"zjCtkkuueD"},{"type":"inlineCode","value":"freqai/prediction_models/ReinforcementLearner","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"zC707fBvdD"},{"type":"text","value":" 中的模板类（或 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"Av6ko9aJpG"},{"type":"inlineCode","value":"user_data/freqaimodels","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"To8L7g8998"},{"type":"text","value":" 下的自定义类）。策略部分则与典型回归器一样，采用","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"AhmSDQTzHc"},{"type":"link","url":"/freqai-feature-engineering","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"特征工程","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"tUnSvKPIP1"}],"urlSource":"freqai-feature-engineering.md","dataUrl":"/freqai-feature-engineering.json","internal":true,"protocol":"file","key":"tIU3bx3K9T"},{"type":"text","value":"的 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"kyvYBNlvk6"},{"type":"inlineCode","value":"feature_engineering_*","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"VYm1D10ZNt"},{"type":"text","value":"。不同之处在于目标的创建，强化学习不需要目标标签。然而，FreqAI 要求在动作列中设置一个默认（中立）值：","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"sQz2kMexzd"}],"key":"hV6EYFAbCl"},{"type":"code","lang":"python","value":"    def set_freqai_targets(self, dataframe, **kwargs) -> DataFrame:\n        \"\"\"\n        *仅适用于启用 FreqAI 的策略*\n        设置模型目标的必需函数。\n        所有目标必须以 `&` 开头，以便 FreqAI 内部识别。\n\n        更多特征工程细节见：\n\n        https://www.freqtrade.io/en/latest/freqai-feature-engineering\n\n        :param df: 将接收目标的策略数据框\n        使用示例：dataframe[\"&-target\"] = dataframe[\"close\"].shift(-1) / dataframe[\"close\"]\n        \"\"\"\n        # 对于 RL，无需设置直接目标。此处为占位（中立），直到智能体发送动作。\n        dataframe[\"&-action\"] = 0\n        return dataframe","position":{"start":{"line":49,"column":1},"end":{"line":66,"column":1}},"key":"FQUxJcsNfc"},{"type":"paragraph","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"children":[{"type":"text","value":"大部分函数与典型回归器相同，但下方函数展示了策略如何将原始价格数据传递给智能体，以便其在训练环境中访问原始 OHLCV：","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"EAjDUyojLw"}],"key":"C0knF8YyvF"},{"type":"code","lang":"python","value":"    def feature_engineering_standard(self, dataframe: DataFrame, **kwargs) -> DataFrame:\n        # 以下特征对 RL 模型是必要的\n        dataframe[f\"%-raw_close\"] = dataframe[\"close\"]\n        dataframe[f\"%-raw_open\"] = dataframe[\"open\"]\n        dataframe[f\"%-raw_high\"] = dataframe[\"high\"]\n        dataframe[f\"%-raw_low\"] = dataframe[\"low\"]\n    return dataframe","position":{"start":{"line":70,"column":1},"end":{"line":78,"column":1}},"key":"aIGOLXGtlB"},{"type":"paragraph","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"children":[{"type":"text","value":"最后，没有显式的\"标签\"需要设置——而是需要分配 ","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"uIMaXlH6To"},{"type":"inlineCode","value":"&-action","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"dZvZyrCC9b"},{"type":"text","value":" 列，该列在 ","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"yIjdvbzJQO"},{"type":"inlineCode","value":"populate_entry/exit_trends()","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"BsEGEUmBtR"},{"type":"text","value":" 中被访问时包含智能体的动作。在本例中，中立动作为 0。此值应与所用环境一致。FreqAI 提供的两个环境均以 0 作为中立动作。","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"th2APk2QfN"}],"key":"dfsXJkuEjG"},{"type":"paragraph","position":{"start":{"line":82,"column":1},"end":{"line":82,"column":1}},"children":[{"type":"text","value":"用户会很快意识到无需设置标签，智能体会\"自主\"做出进出场决策。这使得策略构建变得相当简单。进出场信号由智能体以整数形式给出，直接用于策略中的进出场判断：","position":{"start":{"line":82,"column":1},"end":{"line":82,"column":1}},"key":"MT5JzMck78"}],"key":"d2qyN9lEHl"},{"type":"code","lang":"python","value":"    def populate_entry_trend(self, df: DataFrame, metadata: dict) -> DataFrame:\n\n        enter_long_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 1]\n\n        if enter_long_conditions:\n            df.loc[\n                reduce(lambda x, y: x & y, enter_long_conditions), [\"enter_long\", \"enter_tag\"]\n            ] = (1, \"long\")\n\n        enter_short_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 3]\n\n        if enter_short_conditions:\n            df.loc[\n                reduce(lambda x, y: x & y, enter_short_conditions), [\"enter_short\", \"enter_tag\"]\n            ] = (1, \"short\")\n\n        return df\n\n    def populate_exit_trend(self, df: DataFrame, metadata: dict) -> DataFrame:\n        exit_long_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 2]\n        if exit_long_conditions:\n            df.loc[reduce(lambda x, y: x & y, exit_long_conditions), \"exit_long\"] = 1\n\n        exit_short_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 4]\n        if exit_short_conditions:\n            df.loc[reduce(lambda x, y: x & y, exit_short_conditions), \"exit_short\"] = 1\n\n        return df","position":{"start":{"line":84,"column":1},"end":{"line":113,"column":1}},"key":"xFfbny0wdf"},{"type":"paragraph","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"children":[{"type":"text","value":"需要注意的是，","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"Htv9EuF24W"},{"type":"inlineCode","value":"&-action","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"JyVq3Jmq40"},{"type":"text","value":" 取决于所选环境。上述示例展示了 5 个动作，其中 0 为中立，1 为多头开仓，2 为多头平仓，3 为空头开仓，4 为空头平仓。","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"sGVtTMySLW"}],"key":"WmwvIj89GR"},{"type":"heading","depth":3,"position":{"start":{"line":117,"column":1},"end":{"line":117,"column":1}},"children":[{"type":"text","value":"配置强化学习器","position":{"start":{"line":117,"column":1},"end":{"line":117,"column":1}},"key":"X7QhSswtm7"}],"identifier":"id","label":"配置强化学习器","html_id":"id-4","implicit":true,"key":"R3T1bkFOE9"},{"type":"paragraph","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"children":[{"type":"text","value":"要配置 ","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"XHBP3NAd2R"},{"type":"inlineCode","value":"Reinforcement Learner","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"oPDiJg1rhA"},{"type":"text","value":"，需在 ","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"hkEM6soyKf"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"diwx2mXzC5"},{"type":"text","value":" 配置中包含如下字典：","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"Bqst7Iamej"}],"key":"RPIEgVS9HA"},{"type":"code","lang":"json","value":"        \"rl_config\": {\n            \"train_cycles\": 25,\n            \"add_state_info\": true,\n            \"max_trade_duration_candles\": 300,\n            \"max_training_drawdown_pct\": 0.02,\n            \"cpu_count\": 8,\n            \"model_type\": \"PPO\",\n            \"policy_type\": \"MlpPolicy\",\n            \"model_reward_parameters\": {\n                \"rr\": 1,\n                \"profit_aim\": 0.025\n            }\n        }","position":{"start":{"line":121,"column":1},"end":{"line":135,"column":1}},"key":"xBJyaWg9rr"},{"type":"paragraph","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"参数详情见","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"penoPFywLL"},{"type":"link","url":"/freqai-parameter-table","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"参数表","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"r4Zza2UjB5"}],"urlSource":"freqai-parameter-table.md","dataUrl":"/freqai-parameter-table.json","internal":true,"protocol":"file","key":"tJIu2VjNM7"},{"type":"text","value":"。一般来说，","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"ZgvfsYyrhI"},{"type":"inlineCode","value":"train_cycles","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"y5rSHRJV1g"},{"type":"text","value":" 决定智能体在其人工环境中遍历蜡烛数据以训练模型权重的次数。","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"xbUEIbDIXw"},{"type":"inlineCode","value":"model_type","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"WUg9xtlOAA"},{"type":"text","value":" 是一个字符串，选择 ","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"Tu8DgCaYyo"},{"type":"link","url":"https://stable-baselines3.readthedocs.io/en/master/","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"stable_baselines","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"PwI9c0YXZ3"}],"urlSource":"https://stable-baselines3.readthedocs.io/en/master/","key":"CWv7zVBOGk"},{"type":"text","value":"（外部链接）中可用的模型之一。","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"eXUwiTUoIj"}],"key":"tXgUPQr8xC"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"LjGMICGlNC"}],"key":"GDHYN6GDPE"},{"type":"paragraph","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"children":[{"type":"text","value":"如果你想尝试 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"BSvgTO1Toy"},{"type":"inlineCode","value":"continual_learning","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"z6z3TsGcOi"},{"type":"text","value":"，应在主 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"uROQrB11xg"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"EWxKOJhESV"},{"type":"text","value":" 配置字典中将其设为 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"AAdv0guGw9"},{"type":"inlineCode","value":"true","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"xHYFLSE3hi"},{"type":"text","value":"。这会让强化学习库在每次再训练时，从前一模型的最终状态继续训练新模型，而不是每次都从头训练。","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"qPjB1E546z"}],"key":"RvX5zCXJtw"}],"key":"x8ScK8pBFs"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"SNkqaRTkTE"}],"key":"qPFKEN1sm5"},{"type":"paragraph","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"children":[{"type":"text","value":"请记住，通用的 ","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"cqLxW0dStl"},{"type":"inlineCode","value":"model_training_parameters","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"Dq3QGGsVCB"},{"type":"text","value":" 字典应包含特定 ","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"m811r3uKBB"},{"type":"inlineCode","value":"model_type","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"axycS229V1"},{"type":"text","value":" 的所有模型超参数。例如，","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"qZEdGK2Evz"},{"type":"inlineCode","value":"PPO","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"lyQO2TaOYH"},{"type":"text","value":" 参数见","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"VYejLMq0RM"},{"type":"link","url":"https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"pzAfqZKDYg"}],"urlSource":"https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html","key":"ImsmSMMufM"},{"type":"text","value":"。","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"CT2NZYayu6"}],"key":"PNuSqXmyNq"}],"key":"TyRQL4Mc4C"},{"type":"heading","depth":3,"position":{"start":{"line":147,"column":1},"end":{"line":147,"column":1}},"children":[{"type":"text","value":"创建自定义奖励函数","position":{"start":{"line":147,"column":1},"end":{"line":147,"column":1}},"key":"Uvwjo2Tm3N"}],"identifier":"id","label":"创建自定义奖励函数","html_id":"id-5","implicit":true,"key":"JrBY7hkx9s"},{"type":"admonition","kind":"danger","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"不适用于生产环境","position":{"start":{"line":149,"column":1},"end":{"line":149,"column":1}},"key":"dvW8jzZ6Ap"}],"key":"k8GA1gBkhW"},{"type":"paragraph","position":{"start":{"line":150,"column":1},"end":{"line":150,"column":1}},"children":[{"type":"text","value":"警告！","position":{"start":{"line":150,"column":1},"end":{"line":150,"column":1}},"key":"YtABtjkzcP"}],"key":"XpAK6rkRgZ"},{"type":"paragraph","position":{"start":{"line":152,"column":1},"end":{"line":152,"column":1}},"children":[{"type":"text","value":"Freqtrade 源码中提供的奖励函数仅用于展示功能，旨在展示/测试尽可能多的环境控制特性，并在小型计算机上快速运行。它是基准，不适用于生产环境。请注意，你需要自己创建 custom_reward() 函数，或使用其他用户在 Freqtrade 源码之外构建的模板。","position":{"start":{"line":152,"column":1},"end":{"line":152,"column":1}},"key":"M803gl5h1D"}],"key":"z61SyjW8qR"}],"key":"ShtcQvmj2Z"},{"type":"paragraph","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"当你开始修改策略和预测模型时，会很快发现强化学习器与回归器/分类器有一些重要区别。首先，策略不设置目标值（无标签！）。相反，你需要在 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"JXb9FpMjPO"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"y4htTn9rWx"},{"type":"text","value":" 类中设置 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"gb9ydascsa"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"oWACC4dkAT"},{"type":"text","value":" 函数（见下文）。","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"F0D8kBNhdL"},{"type":"inlineCode","value":"prediction_models/ReinforcementLearner.py","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"kkYWvlo6As"},{"type":"text","value":" 中提供了一个默认的 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"vIGxGVtWea"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"QVTHIMgR3i"},{"type":"text","value":"，用于演示奖励构建的必要模块，但","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"jhpAeNDikd"},{"type":"emphasis","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"不适用于生产","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"xon1IH0KCm"}],"key":"DlJGdUsATm"},{"type":"text","value":"。用户","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"AvOjNIJowz"},{"type":"emphasis","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"必须","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"wfV3HThcVK"}],"key":"BsPrcfuMgu"},{"type":"text","value":"创建自己的自定义强化学习模型类，或使用 Freqtrade 源码之外的预构建模型并保存到 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"yM9PTPWzBj"},{"type":"inlineCode","value":"user_data/freqaimodels","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"FyecmD0K20"},{"type":"text","value":"。在 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"Fakrk3DB3v"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"IdYvJ4oFGJ"},{"type":"text","value":" 中可以表达你对市场的创造性理论。例如，你可以在智能体获利时奖励它，亏损时惩罚它，或奖励其开仓、惩罚其持仓过久。下方展示了这些奖励的计算方式：","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"xZVmuRGHpY"}],"key":"IPiyEIghuk"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"提示","position":{"start":{"line":157,"column":1},"end":{"line":157,"column":1}},"key":"xai8PuI0mz"}],"key":"bgvf6x1L6I"},{"type":"paragraph","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"children":[{"type":"text","value":"最好的奖励函数是连续可微且缩放良好的。换句话说，对罕见事件施加一次性大负惩罚不是好主意，神经网络无法学会这种函数。更好的做法是对常见事件施加小负惩罚，这有助于智能体更快学习。你还可以通过让奖励/惩罚随某些线性/指数函数按严重程度缩放来提升连续性。例如，随着持仓时间增加，逐步增加惩罚，这比在某一时刻一次性施加大惩罚更好。","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"key":"sYLNh33HEQ"}],"key":"lLaF9Yst4Q"}],"key":"WKS7grCGGU"},{"type":"code","lang":"python","value":"from freqtrade.freqai.prediction_models.ReinforcementLearner import ReinforcementLearner\nfrom freqtrade.freqai.RL.Base5ActionRLEnv import Actions, Base5ActionRLEnv, Positions\n\n\nclass MyCoolRLModel(ReinforcementLearner):\n    \"\"\"\n    用户自定义 RL 预测模型。\n\n    将此文件保存到 `freqtrade/user_data/freqaimodels`\n\n    然后用如下命令调用：\n\n    freqtrade trade --freqaimodel MyCoolRLModel --config config.json --strategy SomeCoolStrat\n\n    用户可重写 `IFreqaiModel` 继承树中的任意函数。对 RL 来说，最重要的是在此重写 `MyRLEnv`（见下文），以定义自定义 `calculate_reward()`，或重写环境的其他部分。\n\n    此类还允许用户重写 IFreqaiModel 树的其他部分。例如，可以重写 `def fit()`、`def train()` 或 `def predict()` 以精细控制这些过程。\n\n    另一个常见重写是 `def data_cleaning_predict()`，用于精细控制数据处理管道。\n    \"\"\"\n    class MyRLEnv(Base5ActionRLEnv):\n        \"\"\"\n        用户自定义环境。此类继承自 BaseEnvironment 和 gym.Env。\n        用户可重写父类的任意函数。以下为自定义 `calculate_reward()` 的示例。\n\n        警告！\n        此函数仅用于展示功能，旨在展示尽可能多的环境控制特性，并在小型计算机上快速运行。它是基准，不适用于生产环境。\n        \"\"\"\n        def calculate_reward(self, action: int) -> float:\n            # 首先，若动作无效则惩罚\n            if not self._is_valid(action):\n                return -2\n            pnl = self.get_unrealized_profit()\n\n            factor = 100\n\n            pair = self.pair.replace(':', '')\n\n            # 可使用 dataframe 中的特征值\n            # 假设策略中已生成移位的 RSI 指标。\n            rsi_now = self.raw_features[f\"%-rsi-period_10_shift-1_{pair}_\"\n                            f\"{self.config['timeframe']}\"]\\\n                            .iloc[self._current_tick]\n\n            # 奖励智能体开仓\n            if (action in (Actions.Long_enter.value, Actions.Short_enter.value)\n                    and self._position == Positions.Neutral):\n                if rsi_now < 40:\n                    factor = 40 / rsi_now\n                else:\n                    factor = 1\n                return 25 * factor\n\n            # 惩罚智能体未开仓\n            if action == Actions.Neutral.value and self._position == Positions.Neutral:\n                return -1\n            max_trade_duration = self.rl_config.get('max_trade_duration_candles', 300)\n            trade_duration = self._current_tick - self._last_trade_tick\n            if trade_duration <= max_trade_duration:\n                factor *= 1.5\n            elif trade_duration > max_trade_duration:\n                factor *= 0.5\n            # 惩罚持仓不动\n            if self._position in (Positions.Short, Positions.Long) and \\\n            action == Actions.Neutral.value:\n                return -1 * trade_duration / max_trade_duration\n            # 多头平仓\n            if action == Actions.Long_exit.value and self._position == Positions.Long:\n                if pnl > self.profit_aim * self.rr:\n                    factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n                return float(pnl * factor)\n            # 空头平仓\n            if action == Actions.Short_exit.value and self._position == Positions.Short:\n                if pnl > self.profit_aim * self.rr:\n                    factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n                return float(pnl * factor)\n            return 0.","position":{"start":{"line":161,"column":1},"end":{"line":239,"column":1}},"key":"gipljzBvPW"},{"type":"heading","depth":3,"position":{"start":{"line":241,"column":1},"end":{"line":241,"column":1}},"children":[{"type":"text","value":"使用 Tensorboard","position":{"start":{"line":241,"column":1},"end":{"line":241,"column":1}},"key":"O0tyRzJYCf"}],"identifier":"id-tensorboard","label":"使用 Tensorboard","html_id":"id-tensorboard","implicit":true,"key":"P1nYihmi5D"},{"type":"paragraph","position":{"start":{"line":243,"column":1},"end":{"line":243,"column":1}},"children":[{"type":"text","value":"强化学习模型受益于训练指标的跟踪。FreqAI 集成了 Tensorboard，允许用户跨所有币种和所有再训练过程跟踪训练和评估表现。可通过以下命令激活 Tensorboard：","position":{"start":{"line":243,"column":1},"end":{"line":243,"column":1}},"key":"iOwozfDJ9A"}],"key":"bo6nOyhW7i"},{"type":"code","lang":"bash","value":"tensorboard --logdir user_data/models/unique-id","position":{"start":{"line":245,"column":1},"end":{"line":247,"column":1}},"key":"oc6dMCdx6Y"},{"type":"paragraph","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"children":[{"type":"text","value":"其中 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"orcaQdRvav"},{"type":"inlineCode","value":"unique-id","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"Igoh5PxHf4"},{"type":"text","value":" 是 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"nAKgYMIPUF"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"qbolpHcd7J"},{"type":"text","value":" 配置文件中设置的 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"GbCJiBXUL4"},{"type":"inlineCode","value":"identifier","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"Zn0abRJAFb"},{"type":"text","value":"。该命令需在单独的 shell 中运行，浏览器访问 127.0.0.1:6006（6006 为 Tensorboard 默认端口）查看输出。","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"Q9UUfYNqmZ"}],"key":"ZpEHoClvyu"},{"type":"image","url":"/build/tensorboard-27f8850881840b007f5a56a202abb949.jpg","alt":"tensorboard","position":{"start":{"line":251,"column":1},"end":{"line":251,"column":1}},"key":"WmOn5k55Pt","urlSource":"assets/tensorboard.jpg","urlOptimized":"/build/tensorboard-27f8850881840b007f5a56a202abb949.webp"},{"type":"heading","depth":3,"position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"children":[{"type":"text","value":"自定义日志","position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"key":"l7DsGpzQbE"}],"identifier":"id","label":"自定义日志","html_id":"id-6","implicit":true,"key":"PP5VLAaQ2t"},{"type":"paragraph","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"children":[{"type":"text","value":"FreqAI 还内置了一个名为 ","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"HdMkpUMhg4"},{"type":"inlineCode","value":"self.tensorboard_log","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"bWQZV4yqoX"},{"type":"text","value":" 的分集摘要日志器，用于将自定义信息添加到 Tensorboard 日志。默认情况下，该函数在环境内每步调用一次以记录智能体动作。单集内所有步的值在每集结束时汇总报告，然后所有指标重置为 0，为下一个 episode 做准备。","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"OHmubWnI8s"}],"key":"w2em47c7rd"},{"type":"paragraph","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"children":[{"type":"inlineCode","value":"self.tensorboard_log","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"N2rTb32YFl"},{"type":"text","value":" 也可在环境内任意位置使用，例如可在 ","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"dlub4s6Ln6"},{"type":"inlineCode","value":"calculate_reward","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"N1N8nHTBm4"},{"type":"text","value":" 函数中添加，以收集奖励各部分被调用的频率：","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"nuW4kyQLZD"}],"key":"S0TmsYY8Ft"},{"type":"code","lang":"python","value":"    class MyRLEnv(Base5ActionRLEnv):\n        \"\"\"\n        用户自定义环境。此类继承自 BaseEnvironment 和 gym.Env。\n        用户可重写父类的任意函数。以下为自定义 `calculate_reward()` 的示例。\n        \"\"\"\n        def calculate_reward(self, action: int) -> float:\n            if not self._is_valid(action):\n                self.tensorboard_log(\"invalid\")\n                return -2\n","position":{"start":{"line":259,"column":1},"end":{"line":270,"column":1}},"key":"TNx353u6aM"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"EMgv22qRK3"}],"key":"UEwzP6NcKO"},{"type":"paragraph","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"children":[{"type":"inlineCode","value":"self.tensorboard_log()","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"Qnl1izRbGA"},{"type":"text","value":" 仅用于跟踪计数对象（如事件、动作）。如需记录浮点数，可作为第二参数传入，如 ","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"JvKK4mnKLH"},{"type":"inlineCode","value":"self.tensorboard_log(\"float_metric1\", 0.23)","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"iXlbu2Oa32"},{"type":"text","value":"。此时指标值不会累加。","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"FNHIMskXsc"}],"key":"GNycnPXn92"}],"key":"LHbOpICxPG"},{"type":"heading","depth":3,"position":{"start":{"line":276,"column":1},"end":{"line":276,"column":1}},"children":[{"type":"text","value":"选择基础环境","position":{"start":{"line":276,"column":1},"end":{"line":276,"column":1}},"key":"I7nwpw9MDq"}],"identifier":"id","label":"选择基础环境","html_id":"id-7","implicit":true,"key":"a0yONAvi16"},{"type":"paragraph","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"children":[{"type":"text","value":"FreqAI 提供三种基础环境：","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"fO7BspoeNO"},{"type":"inlineCode","value":"Base3ActionRLEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"kP3W9lvAki"},{"type":"text","value":"、","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"SMWOFei2F6"},{"type":"inlineCode","value":"Base4ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"CEU8JHHHI1"},{"type":"text","value":" 和 ","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"SlJ5G0wCyC"},{"type":"inlineCode","value":"Base5ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"BMEitaA0xu"},{"type":"text","value":"。顾名思义，这些环境分别适用于可选择 3、4、5 种动作的智能体。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"hglruTvea2"},{"type":"inlineCode","value":"Base3ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"iiinRUP8mn"},{"type":"text","value":" 最简单，智能体可选择持有、多头或空头。该环境也可用于仅做多的机器人（自动跟随策略的 ","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"qsbQzsMarq"},{"type":"inlineCode","value":"can_short","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"xDlq6ojBtK"},{"type":"text","value":" 标志），其中多头为开仓，空头为平仓。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"bwX2wWzOh7"},{"type":"inlineCode","value":"Base4ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"Nr8ky27mdk"},{"type":"text","value":" 中，智能体可多头开仓、空头开仓、中立持有或平仓。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"tXkrGfMJg1"},{"type":"inlineCode","value":"Base5ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"B7TYBPp4FS"},{"type":"text","value":" 则与 Base4 类似，但将平仓动作区分为多头平仓和空头平仓。环境选择的主要影响包括：","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"pMbAB84QjJ"}],"key":"pNhhlldYq4"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":280,"column":1},"end":{"line":282,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"children":[{"type":"inlineCode","value":"calculate_reward","position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"key":"s3OuZButW3"},{"type":"text","value":" 中可用的动作","position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"key":"wtaWwt4QS4"}],"key":"b7IlPyqbkf"},{"type":"listItem","spread":true,"position":{"start":{"line":281,"column":1},"end":{"line":282,"column":1}},"children":[{"type":"text","value":"用户策略中消费的动作","position":{"start":{"line":281,"column":1},"end":{"line":281,"column":1}},"key":"v2F8yBVQ0g"}],"key":"yRvSMm575n"}],"key":"gAciAvAqN1"},{"type":"paragraph","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"children":[{"type":"text","value":"FreqAI 提供的所有环境均继承自动作/持仓无关的 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"ZNbvMyB4oQ"},{"type":"inlineCode","value":"BaseEnvironment","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"wB8XutVLUN"},{"type":"text","value":"，该对象包含所有共享逻辑。架构设计易于自定义。最简单的自定义是 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"PBO5eR16kg"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"mdgBbl9g79"},{"type":"text","value":"（详见","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"NoZZ6CIxsz"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"TXuckSpB4l"}],"urlSource":"#creating-a-custom-reward-function","key":"Twzq2MsLmt"},{"type":"text","value":"）。当然，也可进一步扩展环境内的任意函数，只需在预测模型文件的 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"IvIIqmiDys"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"BeAilGbaGY"},{"type":"text","value":" 中重写这些函数即可。更高级的自定义建议直接继承 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"WlgABjgx9v"},{"type":"inlineCode","value":"BaseEnvironment","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"EX9VKHOXQo"},{"type":"text","value":" 创建全新环境。","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"GAXrco95Cb"}],"key":"CRr6q9lOGT"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"WnZleyhg74"}],"key":"FsalsdnHf2"},{"type":"paragraph","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"children":[{"type":"text","value":"仅 ","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"Zqv415cLQI"},{"type":"inlineCode","value":"Base3ActionRLEnv","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"d2ZYy9QPo6"},{"type":"text","value":" 支持仅做多训练/交易（将用户策略属性 ","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"d6ooUjIxV0"},{"type":"inlineCode","value":"can_short = False","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"YDYTEBpsNf"},{"type":"text","value":"）。","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"TT1owZwTGz"}],"key":"S3KpOMV40k"}],"key":"DeFRlsK5ed"}],"key":"ShdQcJZHRX"}],"key":"iPM4OmKNtC"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"FreqAI 运行指南","short_title":"FreqAI 运行","url":"/freqai-running","group":"FreqAI"},"next":{"title":"FreqAI 开发者指南","short_title":"FreqAI 开发","url":"/freqai-developers","group":"FreqAI"}}},"domain":"http://localhost:3001"}