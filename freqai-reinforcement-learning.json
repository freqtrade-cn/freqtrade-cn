{"version":2,"kind":"Article","sha256":"2d94e0012de929dd2ecbcd527816ae7f308f614eceaccf0e07cd87545a95711a","slug":"freqai-reinforcement-learning","location":"/freqai-reinforcement-learning.md","dependencies":[],"frontmatter":{"title":"FreqAI 强化学习指南","description":"本文档详细介绍了 FreqAI 的强化学习功能,包括基本概念、运行方法、环境配置等内容。这些功能可以帮助用户构建和训练强化学习模型进行自动交易。","short_title":"强化学习","subtitle":"强化学习模型的详细说明","subject":"FreqAI 强化学习文档","authors":[{"id":"Freqtrade","name":"Freqtrade"}],"github":"https://github.com/freqtrade-cn/docs_zh-CN","keywords":["Freqtrade","中文文档","交易机器人","量化交易","加密货币","数字货币","区块链","人工智能","机器学习"],"numbering":{"title":{"offset":1}},"edit_url":"https://github.com/freqtrade-cn/docs_zh-CN/blob/main/freqai-reinforcement-learning.md","thumbnail":"/build/tensorboard-27f8850881840b007f5a56a202abb949.jpg","thumbnailOptimized":"/build/tensorboard-27f8850881840b007f5a56a202abb949.webp","exports":[{"format":"md","filename":"freqai-reinforcement-learning.md","url":"/build/freqai-reinforcement-cb72db54ba15f96c69ab38f94769520d.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"强化学习","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"doEnUj3XPN"}],"identifier":"id","label":"强化学习","html_id":"id","implicit":true,"key":"r66VBJKZ8X"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"安装体积","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"TT4a9VDjYE"}],"key":"J3g5nltCFz"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"强化学习依赖项包含如 ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"OazAUezyg4"},{"type":"inlineCode","value":"torch","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"dlfklmafCH"},{"type":"text","value":" 这样的大型包，需在执行 ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"RAS9Lj6HdU"},{"type":"inlineCode","value":"./setup.sh -i","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"aydAkvFMaO"},{"type":"text","value":" 时，在\"Do you also want dependencies for freqai-rl (~700mb additional space required) [y/N]?”问题上选择\"y\"以显式安装。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"rZYXhLWXjB"}],"key":"ymJa0YEwB0"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"喜欢使用 docker 的用户应确保使用带有 ","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"VvLEcrvCua"},{"type":"inlineCode","value":"_freqairl","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"QpCk060qqp"},{"type":"text","value":" 后缀的 docker 镜像。","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"KEDj56cfoK"}],"key":"cpoe19N246"}],"key":"sC5ETu5u2D"},{"type":"heading","depth":3,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"背景与术语","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"UDGYxwFzaC"}],"identifier":"id","label":"背景与术语","html_id":"id-1","implicit":true,"key":"W5TbEf0aJT"},{"type":"heading","depth":4,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"什么是 RL，FreqAI 为什么需要它？","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"mL3pAjChrd"}],"identifier":"id-rl-freqai","label":"什么是 RL，FreqAI 为什么需要它？","html_id":"id-rl-freqai","implicit":true,"key":"GK9CffdE2U"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"强化学习涉及两个重要组成部分：","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"vsP5EfGABE"},{"type":"emphasis","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"智能体（agent）","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"POlnRznNft"},{"type":"emphasis","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"和训练","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"DX5cht5fQX"}],"key":"ukNKY7HjYR"},{"type":"text","value":"环境（environment）","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"Fz4zdocVVz"}],"key":"cvDQsDYm92"},{"type":"text","value":"。在智能体训练期间，智能体逐根遍历历史蜡烛数据，每次做出一组动作中的一个：多头开仓、多头平仓、空头开仓、空头平仓、中立。在此训练过程中，环境会跟踪这些动作的表现，并根据自定义的 ","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"ABCBPD3tUE"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"VjbAlcQphv"},{"type":"text","value":"（我们为用户提供了一个默认奖励函数，详情见","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"vWiz4uULDD"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"x5efoshn6A"}],"urlSource":"#creating-a-custom-reward-function","key":"c8xdTybzeF"},{"type":"text","value":"）对智能体进行奖励。奖励用于训练神经网络中的权重。","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"YSNDzOxM9i"}],"key":"RebdKWOrQx"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"FreqAI RL 实现的另一个重要组成部分是*状态（state）*信息的使用。每一步都会将状态信息（如当前利润、当前持仓、当前交易持续时间）输入网络。这些信息用于训练环境中的智能体，并在 dry/live 时强化智能体（此功能在回测中不可用）。","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"xLMTKClwgZ"},{"type":"emphasis","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"FreqAI + Freqtrade 是这种强化机制的完美结合，因为这些信息在实时部署中随时可用。","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"a0Ufgupwxk"}],"key":"VnDpt0Gkz1"}],"key":"shMHHQDtAt"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"强化学习是 FreqAI 的自然进化，因为它为市场自适应和反应性增加了新的层次，这是分类器和回归器无法比拟的。然而，分类器和回归器也有 RL 不具备的优势，比如稳健的预测。训练不当的 RL 智能体可能会找到\"漏洞\"或\"技巧\"来最大化奖励，但实际上并未获得任何交易收益。因此，RL 更加复杂，需要比典型分类器和回归器更高的理解水平。","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"FJy6juY4SW"}],"key":"WE0rdJzW2Y"},{"type":"heading","depth":4,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"RL 接口","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"paabc18s48"}],"identifier":"rl","label":"RL 接口","html_id":"rl","implicit":true,"key":"vvuB22Gm6j"},{"type":"paragraph","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"在当前框架下，我们旨在通过通用的\"预测模型\"文件暴露训练环境，该文件是用户继承的 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"nXFGqrg2XW"},{"type":"inlineCode","value":"BaseReinforcementLearner","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"fyPjrFGKeH"},{"type":"text","value":" 对象（如 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"nxEYhoj14h"},{"type":"inlineCode","value":"freqai/prediction_models/ReinforcementLearner","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"vKB8x6B4kf"},{"type":"text","value":"）。在此用户类中，RL 环境可通过 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"yXcUYEvojE"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"w8X5pIORG6"},{"type":"text","value":" 进行自定义（见","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"ZMWmxb2328"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"下文","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"JojbKGxM8X"}],"urlSource":"#creating-a-custom-reward-function","key":"SHJ5TY4NCc"},{"type":"text","value":"）。","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"FbtQPaAKOr"}],"key":"a0cziTfwwD"},{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"我们设想大多数用户会将精力集中在创造性设计 ","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"d6zcxXwcaz"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"xwIecbJqqc"},{"type":"text","value":" 函数（详情见","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"qjGZxcO5ik"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"VJE7BVjNXh"}],"urlSource":"#creating-a-custom-reward-function","key":"Be5TiG9l5E"},{"type":"text","value":"），而对环境的其他部分保持不变。其他用户甚至不会修改环境，只会调整配置和 FreqAI 已有的强大特征工程。与此同时，我们也允许高级用户完全自定义自己的模型类。","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"IETzKbyCkB"}],"key":"t5VJOGE890"},{"type":"paragraph","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"该框架基于 stable_baselines3（torch）和 OpenAI gym 构建基础环境类。但总体而言，模型类隔离良好，因此可以轻松集成其他竞争库。环境继承自 ","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"ixguLPBJ9T"},{"type":"inlineCode","value":"gym.Env","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"EHHXLsdzLo"},{"type":"text","value":"，因此如需切换到其他库，需编写全新的环境。","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"dG6NW3KHag"}],"key":"jUKPYqjvSn"},{"type":"heading","depth":4,"position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"重要注意事项","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"V27wFqKpte"}],"identifier":"id","label":"重要注意事项","html_id":"id-2","implicit":true,"key":"V33oyISOBa"},{"type":"paragraph","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"如上所述，智能体在一个\"人工\"交易\"环境\"中被\"训练\"。在我们的案例中，这个环境看起来与真实的 Freqtrade 回测环境很相似，但它","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"G7X9eioJTv"},{"type":"emphasis","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"不是","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"IGztONosMb"}],"key":"YwR9jwIicg"},{"type":"text","value":"。实际上，RL 训练环境要简单得多。它不包含任何复杂的策略逻辑，如 ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"ZzdX6Ld96W"},{"type":"inlineCode","value":"custom_exit","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"kx1J9tm7Od"},{"type":"text","value":"、","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"OVNaYsa7TY"},{"type":"inlineCode","value":"custom_stoploss","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"KI38KbKoy1"},{"type":"text","value":"、杠杆控制等回调。RL 环境是对真实市场的非常\"原始\"表示，智能体可以自由学习由 ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"k8y4VUhpCm"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"g3bGXMX7Bz"},{"type":"text","value":" 强制执行的策略（如止损、止盈等）。因此，必须注意，智能体训练环境并不等同于真实世界。","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"p4ZY9UoLQt"}],"key":"Ni1O4H1tXJ"},{"type":"heading","depth":3,"position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"运行强化学习","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"l6bN9kvps0"}],"identifier":"id","label":"运行强化学习","html_id":"id-3","implicit":true,"key":"HcZI1Lutyr"},{"type":"paragraph","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"text","value":"设置和运行强化学习模型与运行回归器或分类器相同。命令行上必须定义同样的两个参数：","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"pxzFn1XEQQ"},{"type":"inlineCode","value":"--freqaimodel","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"ehzieBcuwD"},{"type":"text","value":" 和 ","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"Y66lnFDmxX"},{"type":"inlineCode","value":"--strategy","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"tNE6TSXEsv"},{"type":"text","value":"：","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"Dz63YUbvEa"}],"key":"k0xGJfYgtO"},{"type":"code","lang":"bash","value":"freqtrade trade --freqaimodel ReinforcementLearner --strategy MyRLStrategy --config config.json","position":{"start":{"line":43,"column":1},"end":{"line":45,"column":1}},"key":"AosYLsMqf4"},{"type":"paragraph","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"其中 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"sKoIgtt8TA"},{"type":"inlineCode","value":"ReinforcementLearner","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"Pv6ZpzEAup"},{"type":"text","value":" 将使用 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"EVL7Zw8FBZ"},{"type":"inlineCode","value":"freqai/prediction_models/ReinforcementLearner","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"CGlN1WlRFl"},{"type":"text","value":" 中的模板类（或 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"WEPyNSRliP"},{"type":"inlineCode","value":"user_data/freqaimodels","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"HBue4Ns6uF"},{"type":"text","value":" 下的自定义类）。策略部分则与典型回归器一样，采用","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"Kk1I317VMq"},{"type":"link","url":"/freqai-feature-engineering","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"特征工程","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"woxfJV0mcx"}],"urlSource":"freqai-feature-engineering.md","dataUrl":"/freqai-feature-engineering.json","internal":true,"protocol":"file","key":"cuGuUV8jIc"},{"type":"text","value":"的 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"XIaT7Dpsax"},{"type":"inlineCode","value":"feature_engineering_*","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"AVmlPzyG6e"},{"type":"text","value":"。不同之处在于目标的创建，强化学习不需要目标标签。然而，FreqAI 要求在动作列中设置一个默认（中立）值：","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"sT9hZq0U4w"}],"key":"Dc3wokvcyP"},{"type":"code","lang":"python","value":"    def set_freqai_targets(self, dataframe, **kwargs) -> DataFrame:\n        \"\"\"\n        *仅适用于启用 FreqAI 的策略*\n        设置模型目标的必需函数。\n        所有目标必须以 `&` 开头，以便 FreqAI 内部识别。\n\n        更多特征工程细节见：\n\n        https://www.freqtrade.io/en/latest/freqai-feature-engineering\n\n        :param df: 将接收目标的策略数据框\n        使用示例：dataframe[\"&-target\"] = dataframe[\"close\"].shift(-1) / dataframe[\"close\"]\n        \"\"\"\n        # 对于 RL，无需设置直接目标。此处为占位（中立），直到智能体发送动作。\n        dataframe[\"&-action\"] = 0\n        return dataframe","position":{"start":{"line":49,"column":1},"end":{"line":66,"column":1}},"key":"U9wy3elxLj"},{"type":"paragraph","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"children":[{"type":"text","value":"大部分函数与典型回归器相同，但下方函数展示了策略如何将原始价格数据传递给智能体，以便其在训练环境中访问原始 OHLCV：","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"pKXiGUSNyl"}],"key":"LrpmxF3BJW"},{"type":"code","lang":"python","value":"    def feature_engineering_standard(self, dataframe: DataFrame, **kwargs) -> DataFrame:\n        # 以下特征对 RL 模型是必要的\n        dataframe[f\"%-raw_close\"] = dataframe[\"close\"]\n        dataframe[f\"%-raw_open\"] = dataframe[\"open\"]\n        dataframe[f\"%-raw_high\"] = dataframe[\"high\"]\n        dataframe[f\"%-raw_low\"] = dataframe[\"low\"]\n    return dataframe","position":{"start":{"line":70,"column":1},"end":{"line":78,"column":1}},"key":"md03jZHSrw"},{"type":"paragraph","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"children":[{"type":"text","value":"最后，没有显式的\"标签\"需要设置——而是需要分配 ","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"XVykAatXWM"},{"type":"inlineCode","value":"&-action","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"HUcTlnZosv"},{"type":"text","value":" 列，该列在 ","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"kACQlz66Hz"},{"type":"inlineCode","value":"populate_entry/exit_trends()","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"E6r31G5boc"},{"type":"text","value":" 中被访问时包含智能体的动作。在本例中，中立动作为 0。此值应与所用环境一致。FreqAI 提供的两个环境均以 0 作为中立动作。","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"v5zuUA3SnG"}],"key":"RriPYDYxsG"},{"type":"paragraph","position":{"start":{"line":82,"column":1},"end":{"line":82,"column":1}},"children":[{"type":"text","value":"用户会很快意识到无需设置标签，智能体会\"自主\"做出进出场决策。这使得策略构建变得相当简单。进出场信号由智能体以整数形式给出，直接用于策略中的进出场判断：","position":{"start":{"line":82,"column":1},"end":{"line":82,"column":1}},"key":"y7DrzSI3xP"}],"key":"scppsbdobE"},{"type":"code","lang":"python","value":"    def populate_entry_trend(self, df: DataFrame, metadata: dict) -> DataFrame:\n\n        enter_long_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 1]\n\n        if enter_long_conditions:\n            df.loc[\n                reduce(lambda x, y: x & y, enter_long_conditions), [\"enter_long\", \"enter_tag\"]\n            ] = (1, \"long\")\n\n        enter_short_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 3]\n\n        if enter_short_conditions:\n            df.loc[\n                reduce(lambda x, y: x & y, enter_short_conditions), [\"enter_short\", \"enter_tag\"]\n            ] = (1, \"short\")\n\n        return df\n\n    def populate_exit_trend(self, df: DataFrame, metadata: dict) -> DataFrame:\n        exit_long_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 2]\n        if exit_long_conditions:\n            df.loc[reduce(lambda x, y: x & y, exit_long_conditions), \"exit_long\"] = 1\n\n        exit_short_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 4]\n        if exit_short_conditions:\n            df.loc[reduce(lambda x, y: x & y, exit_short_conditions), \"exit_short\"] = 1\n\n        return df","position":{"start":{"line":84,"column":1},"end":{"line":113,"column":1}},"key":"urJf2O1bMA"},{"type":"paragraph","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"children":[{"type":"text","value":"需要注意的是，","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"zd5pm8KMcA"},{"type":"inlineCode","value":"&-action","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"Q1BsxQIwi3"},{"type":"text","value":" 取决于所选环境。上述示例展示了 5 个动作，其中 0 为中立，1 为多头开仓，2 为多头平仓，3 为空头开仓，4 为空头平仓。","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"frgUHmGFka"}],"key":"WSsCKWNU9v"},{"type":"heading","depth":3,"position":{"start":{"line":117,"column":1},"end":{"line":117,"column":1}},"children":[{"type":"text","value":"配置强化学习器","position":{"start":{"line":117,"column":1},"end":{"line":117,"column":1}},"key":"AGPkV0cZGw"}],"identifier":"id","label":"配置强化学习器","html_id":"id-4","implicit":true,"key":"MGsfsoE5VC"},{"type":"paragraph","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"children":[{"type":"text","value":"要配置 ","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"ubkiqCGNUp"},{"type":"inlineCode","value":"Reinforcement Learner","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"LA40o2iQmS"},{"type":"text","value":"，需在 ","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"OqPDejTnmB"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"Q395MbptPF"},{"type":"text","value":" 配置中包含如下字典：","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"xa9HdVKDuk"}],"key":"JqbFwrnL3U"},{"type":"code","lang":"json","value":"        \"rl_config\": {\n            \"train_cycles\": 25,\n            \"add_state_info\": true,\n            \"max_trade_duration_candles\": 300,\n            \"max_training_drawdown_pct\": 0.02,\n            \"cpu_count\": 8,\n            \"model_type\": \"PPO\",\n            \"policy_type\": \"MlpPolicy\",\n            \"model_reward_parameters\": {\n                \"rr\": 1,\n                \"profit_aim\": 0.025\n            }\n        }","position":{"start":{"line":121,"column":1},"end":{"line":135,"column":1}},"key":"sw45J0vkZ0"},{"type":"paragraph","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"参数详情见","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"KudmsmRSE4"},{"type":"link","url":"/freqai-parameter-table","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"参数表","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"mghqrohgGM"}],"urlSource":"freqai-parameter-table.md","dataUrl":"/freqai-parameter-table.json","internal":true,"protocol":"file","key":"hcCrkwiomi"},{"type":"text","value":"。一般来说，","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"CpxbIeeBfk"},{"type":"inlineCode","value":"train_cycles","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"iGohJOB65s"},{"type":"text","value":" 决定智能体在其人工环境中遍历蜡烛数据以训练模型权重的次数。","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"do9TE7BEBj"},{"type":"inlineCode","value":"model_type","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"uwGOHnivkh"},{"type":"text","value":" 是一个字符串，选择 ","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"ixvZ9vjtco"},{"type":"link","url":"https://stable-baselines3.readthedocs.io/en/master/","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"stable_baselines","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"stfpK5kmTF"}],"urlSource":"https://stable-baselines3.readthedocs.io/en/master/","key":"G7me5wL406"},{"type":"text","value":"（外部链接）中可用的模型之一。","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"ouaDqZViNR"}],"key":"TLA7n0bVpm"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"iCDNIfnvSz"}],"key":"QRFgHL7GJN"},{"type":"paragraph","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"children":[{"type":"text","value":"如果你想尝试 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"j6RJ7BDxhQ"},{"type":"inlineCode","value":"continual_learning","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"XpLaMRTrET"},{"type":"text","value":"，应在主 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"MC9SG6zPZu"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"hdxiYI8OAF"},{"type":"text","value":" 配置字典中将其设为 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"urBaCGuWdK"},{"type":"inlineCode","value":"true","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"RLKgb5ntXp"},{"type":"text","value":"。这会让强化学习库在每次再训练时，从前一模型的最终状态继续训练新模型，而不是每次都从头训练。","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"i5KDzMFDDw"}],"key":"NSHshhhOk0"}],"key":"vHI7qVnruo"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"rqFmV01JzP"}],"key":"Fm2rb4eH3b"},{"type":"paragraph","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"children":[{"type":"text","value":"请记住，通用的 ","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"iOpFnQqhQG"},{"type":"inlineCode","value":"model_training_parameters","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"Bcaowkg23I"},{"type":"text","value":" 字典应包含特定 ","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"ulMJVcsxBU"},{"type":"inlineCode","value":"model_type","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"HbPO4h3o2f"},{"type":"text","value":" 的所有模型超参数。例如，","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"pWQNmFbvdL"},{"type":"inlineCode","value":"PPO","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"H4p0BZG0I0"},{"type":"text","value":" 参数见","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"lOM5x4gMYp"},{"type":"link","url":"https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"tIfHmRvQj4"}],"urlSource":"https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html","key":"mSfle12evZ"},{"type":"text","value":"。","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"NJ4k4uQ7pu"}],"key":"BUd18iSAfM"}],"key":"vQjz8HKXJj"},{"type":"heading","depth":3,"position":{"start":{"line":147,"column":1},"end":{"line":147,"column":1}},"children":[{"type":"text","value":"创建自定义奖励函数","position":{"start":{"line":147,"column":1},"end":{"line":147,"column":1}},"key":"PCuodnJkNP"}],"identifier":"id","label":"创建自定义奖励函数","html_id":"id-5","implicit":true,"key":"BjyXwsx3Xd"},{"type":"admonition","kind":"danger","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"不适用于生产环境","position":{"start":{"line":149,"column":1},"end":{"line":149,"column":1}},"key":"FV3lEJ6GcE"}],"key":"E7AXDz491Q"},{"type":"paragraph","position":{"start":{"line":150,"column":1},"end":{"line":150,"column":1}},"children":[{"type":"text","value":"警告！","position":{"start":{"line":150,"column":1},"end":{"line":150,"column":1}},"key":"DP6kzYIALo"}],"key":"jCvyR6iDM7"},{"type":"paragraph","position":{"start":{"line":152,"column":1},"end":{"line":152,"column":1}},"children":[{"type":"text","value":"Freqtrade 源码中提供的奖励函数仅用于展示功能，旨在展示/测试尽可能多的环境控制特性，并在小型计算机上快速运行。它是基准，不适用于生产环境。请注意，你需要自己创建 custom_reward() 函数，或使用其他用户在 Freqtrade 源码之外构建的模板。","position":{"start":{"line":152,"column":1},"end":{"line":152,"column":1}},"key":"gbyrYibR6z"}],"key":"jmmTMu34Om"}],"key":"wGn57Pthru"},{"type":"paragraph","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"当你开始修改策略和预测模型时，会很快发现强化学习器与回归器/分类器有一些重要区别。首先，策略不设置目标值（无标签！）。相反，你需要在 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"SKx4k3QAtl"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"Kfmel43Olr"},{"type":"text","value":" 类中设置 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"TVcnRFR0HN"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"BJzsC7yqeD"},{"type":"text","value":" 函数（见下文）。","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"uRUrCkcqjE"},{"type":"inlineCode","value":"prediction_models/ReinforcementLearner.py","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"ug1bhVCGjS"},{"type":"text","value":" 中提供了一个默认的 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"qQv3OgNuyu"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"uc9vKDMVID"},{"type":"text","value":"，用于演示奖励构建的必要模块，但","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"GBv6n0G1bF"},{"type":"emphasis","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"不适用于生产","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"knMRk2N14g"}],"key":"HmjwtJJCrN"},{"type":"text","value":"。用户","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"q4aXdbFtaO"},{"type":"emphasis","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"必须","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"e0pzon8zO6"}],"key":"zIbnrwLf9C"},{"type":"text","value":"创建自己的自定义强化学习模型类，或使用 Freqtrade 源码之外的预构建模型并保存到 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"anvHOJI7mh"},{"type":"inlineCode","value":"user_data/freqaimodels","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"tTVpaGe3QC"},{"type":"text","value":"。在 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"zjxEx7mhZz"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"iayUoFnyXu"},{"type":"text","value":" 中可以表达你对市场的创造性理论。例如，你可以在智能体获利时奖励它，亏损时惩罚它，或奖励其开仓、惩罚其持仓过久。下方展示了这些奖励的计算方式：","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"EYxPABHRhf"}],"key":"xyByaRCMHU"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"提示","position":{"start":{"line":157,"column":1},"end":{"line":157,"column":1}},"key":"iPVa9pD4LL"}],"key":"EfRVbjgETo"},{"type":"paragraph","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"children":[{"type":"text","value":"最好的奖励函数是连续可微且缩放良好的。换句话说，对罕见事件施加一次性大负惩罚不是好主意，神经网络无法学会这种函数。更好的做法是对常见事件施加小负惩罚，这有助于智能体更快学习。你还可以通过让奖励/惩罚随某些线性/指数函数按严重程度缩放来提升连续性。例如，随着持仓时间增加，逐步增加惩罚，这比在某一时刻一次性施加大惩罚更好。","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"key":"SKY74xCrZw"}],"key":"Pkmxcf31Pd"}],"key":"w2OsvmPMXv"},{"type":"code","lang":"python","value":"from freqtrade.freqai.prediction_models.ReinforcementLearner import ReinforcementLearner\nfrom freqtrade.freqai.RL.Base5ActionRLEnv import Actions, Base5ActionRLEnv, Positions\n\n\nclass MyCoolRLModel(ReinforcementLearner):\n    \"\"\"\n    用户自定义 RL 预测模型。\n\n    将此文件保存到 `freqtrade/user_data/freqaimodels`\n\n    然后用如下命令调用：\n\n    freqtrade trade --freqaimodel MyCoolRLModel --config config.json --strategy SomeCoolStrat\n\n    用户可重写 `IFreqaiModel` 继承树中的任意函数。对 RL 来说，最重要的是在此重写 `MyRLEnv`（见下文），以定义自定义 `calculate_reward()`，或重写环境的其他部分。\n\n    此类还允许用户重写 IFreqaiModel 树的其他部分。例如，可以重写 `def fit()`、`def train()` 或 `def predict()` 以精细控制这些过程。\n\n    另一个常见重写是 `def data_cleaning_predict()`，用于精细控制数据处理管道。\n    \"\"\"\n    class MyRLEnv(Base5ActionRLEnv):\n        \"\"\"\n        用户自定义环境。此类继承自 BaseEnvironment 和 gym.Env。\n        用户可重写父类的任意函数。以下为自定义 `calculate_reward()` 的示例。\n\n        警告！\n        此函数仅用于展示功能，旨在展示尽可能多的环境控制特性，并在小型计算机上快速运行。它是基准，不适用于生产环境。\n        \"\"\"\n        def calculate_reward(self, action: int) -> float:\n            # 首先，若动作无效则惩罚\n            if not self._is_valid(action):\n                return -2\n            pnl = self.get_unrealized_profit()\n\n            factor = 100\n\n            pair = self.pair.replace(':', '')\n\n            # 可使用 dataframe 中的特征值\n            # 假设策略中已生成移位的 RSI 指标。\n            rsi_now = self.raw_features[f\"%-rsi-period_10_shift-1_{pair}_\"\n                            f\"{self.config['timeframe']}\"]\\\n                            .iloc[self._current_tick]\n\n            # 奖励智能体开仓\n            if (action in (Actions.Long_enter.value, Actions.Short_enter.value)\n                    and self._position == Positions.Neutral):\n                if rsi_now < 40:\n                    factor = 40 / rsi_now\n                else:\n                    factor = 1\n                return 25 * factor\n\n            # 惩罚智能体未开仓\n            if action == Actions.Neutral.value and self._position == Positions.Neutral:\n                return -1\n            max_trade_duration = self.rl_config.get('max_trade_duration_candles', 300)\n            trade_duration = self._current_tick - self._last_trade_tick\n            if trade_duration <= max_trade_duration:\n                factor *= 1.5\n            elif trade_duration > max_trade_duration:\n                factor *= 0.5\n            # 惩罚持仓不动\n            if self._position in (Positions.Short, Positions.Long) and \\\n            action == Actions.Neutral.value:\n                return -1 * trade_duration / max_trade_duration\n            # 多头平仓\n            if action == Actions.Long_exit.value and self._position == Positions.Long:\n                if pnl > self.profit_aim * self.rr:\n                    factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n                return float(pnl * factor)\n            # 空头平仓\n            if action == Actions.Short_exit.value and self._position == Positions.Short:\n                if pnl > self.profit_aim * self.rr:\n                    factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n                return float(pnl * factor)\n            return 0.","position":{"start":{"line":161,"column":1},"end":{"line":239,"column":1}},"key":"C738dAO7iA"},{"type":"heading","depth":3,"position":{"start":{"line":241,"column":1},"end":{"line":241,"column":1}},"children":[{"type":"text","value":"使用 Tensorboard","position":{"start":{"line":241,"column":1},"end":{"line":241,"column":1}},"key":"eAnzytbxdK"}],"identifier":"id-tensorboard","label":"使用 Tensorboard","html_id":"id-tensorboard","implicit":true,"key":"nT46BedtDj"},{"type":"paragraph","position":{"start":{"line":243,"column":1},"end":{"line":243,"column":1}},"children":[{"type":"text","value":"强化学习模型受益于训练指标的跟踪。FreqAI 集成了 Tensorboard，允许用户跨所有币种和所有再训练过程跟踪训练和评估表现。可通过以下命令激活 Tensorboard：","position":{"start":{"line":243,"column":1},"end":{"line":243,"column":1}},"key":"YQ9ApfWA2m"}],"key":"e0BSTQj8wH"},{"type":"code","lang":"bash","value":"tensorboard --logdir user_data/models/unique-id","position":{"start":{"line":245,"column":1},"end":{"line":247,"column":1}},"key":"ZqH5XNgaNY"},{"type":"paragraph","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"children":[{"type":"text","value":"其中 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"oyAU5kESGC"},{"type":"inlineCode","value":"unique-id","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"WUWbe6YmcO"},{"type":"text","value":" 是 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"A8i4Kd5yso"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"UZ5bJq1Fca"},{"type":"text","value":" 配置文件中设置的 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"PoTez5jh9y"},{"type":"inlineCode","value":"identifier","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"IgNUh9WiEJ"},{"type":"text","value":"。该命令需在单独的 shell 中运行，浏览器访问 127.0.0.1:6006（6006 为 Tensorboard 默认端口）查看输出。","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"pdCBcrVGJm"}],"key":"DRxPOfqxKh"},{"type":"image","url":"/build/tensorboard-27f8850881840b007f5a56a202abb949.jpg","alt":"tensorboard","position":{"start":{"line":251,"column":1},"end":{"line":251,"column":1}},"key":"X9RFYmTWvl","urlSource":"assets/tensorboard.jpg","urlOptimized":"/build/tensorboard-27f8850881840b007f5a56a202abb949.webp"},{"type":"heading","depth":3,"position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"children":[{"type":"text","value":"自定义日志","position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"key":"rA6Zxy8OS8"}],"identifier":"id","label":"自定义日志","html_id":"id-6","implicit":true,"key":"JIK9XgIjYL"},{"type":"paragraph","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"children":[{"type":"text","value":"FreqAI 还内置了一个名为 ","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"piewIuBeyk"},{"type":"inlineCode","value":"self.tensorboard_log","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"EXXw4fCujc"},{"type":"text","value":" 的分集摘要日志器，用于将自定义信息添加到 Tensorboard 日志。默认情况下，该函数在环境内每步调用一次以记录智能体动作。单集内所有步的值在每集结束时汇总报告，然后所有指标重置为 0，为下一个 episode 做准备。","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"nv3yNh632a"}],"key":"yT401h5oTk"},{"type":"paragraph","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"children":[{"type":"inlineCode","value":"self.tensorboard_log","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"Y5mXHXaYkS"},{"type":"text","value":" 也可在环境内任意位置使用，例如可在 ","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"mKmCBNyvIp"},{"type":"inlineCode","value":"calculate_reward","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"puPvaj5QFT"},{"type":"text","value":" 函数中添加，以收集奖励各部分被调用的频率：","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"fjwEWqI0JA"}],"key":"BQuT5mkY1e"},{"type":"code","lang":"python","value":"    class MyRLEnv(Base5ActionRLEnv):\n        \"\"\"\n        用户自定义环境。此类继承自 BaseEnvironment 和 gym.Env。\n        用户可重写父类的任意函数。以下为自定义 `calculate_reward()` 的示例。\n        \"\"\"\n        def calculate_reward(self, action: int) -> float:\n            if not self._is_valid(action):\n                self.tensorboard_log(\"invalid\")\n                return -2\n","position":{"start":{"line":259,"column":1},"end":{"line":270,"column":1}},"key":"jc54b82Jqk"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"o4fZXuDaZu"}],"key":"kANGnLMFjF"},{"type":"paragraph","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"children":[{"type":"inlineCode","value":"self.tensorboard_log()","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"zLfNakuvyv"},{"type":"text","value":" 仅用于跟踪计数对象（如事件、动作）。如需记录浮点数，可作为第二参数传入，如 ","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"iOd2XRMBwX"},{"type":"inlineCode","value":"self.tensorboard_log(\"float_metric1\", 0.23)","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"pS43X4Eth1"},{"type":"text","value":"。此时指标值不会累加。","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"fvpQXFDsrC"}],"key":"n7EgAlz2gW"}],"key":"muF69sP014"},{"type":"heading","depth":3,"position":{"start":{"line":276,"column":1},"end":{"line":276,"column":1}},"children":[{"type":"text","value":"选择基础环境","position":{"start":{"line":276,"column":1},"end":{"line":276,"column":1}},"key":"NmlUW2GMnR"}],"identifier":"id","label":"选择基础环境","html_id":"id-7","implicit":true,"key":"CZvlPAxwhg"},{"type":"paragraph","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"children":[{"type":"text","value":"FreqAI 提供三种基础环境：","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"l7AnoFYAdj"},{"type":"inlineCode","value":"Base3ActionRLEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"pq58UAQ5w4"},{"type":"text","value":"、","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"QSTN0K3ZxX"},{"type":"inlineCode","value":"Base4ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"gyL5zfV3yp"},{"type":"text","value":" 和 ","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"o6pXUPBiVc"},{"type":"inlineCode","value":"Base5ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"VzUyvLXV1J"},{"type":"text","value":"。顾名思义，这些环境分别适用于可选择 3、4、5 种动作的智能体。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"OlXxIMDJsD"},{"type":"inlineCode","value":"Base3ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"GEzEjC3HaT"},{"type":"text","value":" 最简单，智能体可选择持有、多头或空头。该环境也可用于仅做多的机器人（自动跟随策略的 ","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"RKJvTGcsJA"},{"type":"inlineCode","value":"can_short","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"CDhDk1USVB"},{"type":"text","value":" 标志），其中多头为开仓，空头为平仓。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"ii8UHX8Kw3"},{"type":"inlineCode","value":"Base4ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"JKuaLyEC39"},{"type":"text","value":" 中，智能体可多头开仓、空头开仓、中立持有或平仓。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"fuFDPpZhDb"},{"type":"inlineCode","value":"Base5ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"cygfIwwLV2"},{"type":"text","value":" 则与 Base4 类似，但将平仓动作区分为多头平仓和空头平仓。环境选择的主要影响包括：","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"iQzteW1JGk"}],"key":"cCWJYkczPZ"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":280,"column":1},"end":{"line":282,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"children":[{"type":"inlineCode","value":"calculate_reward","position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"key":"PZGMld2ryz"},{"type":"text","value":" 中可用的动作","position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"key":"rzTep68PRG"}],"key":"y1sJr46ziB"},{"type":"listItem","spread":true,"position":{"start":{"line":281,"column":1},"end":{"line":282,"column":1}},"children":[{"type":"text","value":"用户策略中消费的动作","position":{"start":{"line":281,"column":1},"end":{"line":281,"column":1}},"key":"pjRfkaDJKz"}],"key":"dnTFxSxoYa"}],"key":"osxb7OAfIA"},{"type":"paragraph","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"children":[{"type":"text","value":"FreqAI 提供的所有环境均继承自动作/持仓无关的 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"oUtXm8i4OT"},{"type":"inlineCode","value":"BaseEnvironment","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"QnGK5v2Ggs"},{"type":"text","value":"，该对象包含所有共享逻辑。架构设计易于自定义。最简单的自定义是 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"P6KGAH1Iq5"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"OjzViuwpTA"},{"type":"text","value":"（详见","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"BvW1eXlpLZ"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"SyzXY1KGg3"}],"urlSource":"#creating-a-custom-reward-function","key":"xuoOM0tLi3"},{"type":"text","value":"）。当然，也可进一步扩展环境内的任意函数，只需在预测模型文件的 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"UfkQhVLUEq"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"IJ2nWPA2Ro"},{"type":"text","value":" 中重写这些函数即可。更高级的自定义建议直接继承 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"IXTYQ9GVf3"},{"type":"inlineCode","value":"BaseEnvironment","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"pwvXPPAKRl"},{"type":"text","value":" 创建全新环境。","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"kr4aO6X0MR"}],"key":"SfRsZCRwPI"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"bDTGpMCVDg"}],"key":"yiHpQpK3ar"},{"type":"paragraph","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"children":[{"type":"text","value":"仅 ","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"Q0IptqjKQA"},{"type":"inlineCode","value":"Base3ActionRLEnv","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"GlIqCDklNh"},{"type":"text","value":" 支持仅做多训练/交易（将用户策略属性 ","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"EdZ3OOr8Bh"},{"type":"inlineCode","value":"can_short = False","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"TCQrOk9cER"},{"type":"text","value":"）。","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"MVY7rOKQKL"}],"key":"Z0ruNtJVOJ"}],"key":"kG3ipbjOsO"}],"key":"QVIRW44LzP"}],"key":"pNwKKh9fPH"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"FreqAI 运行指南","short_title":"FreqAI 运行","url":"/freqai-running","group":"FreqAI"},"next":{"title":"FreqAI 开发者指南","short_title":"FreqAI 开发","url":"/freqai-developers","group":"FreqAI"}}},"domain":"http://localhost:3002"}