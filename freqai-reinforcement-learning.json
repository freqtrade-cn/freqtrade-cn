{"version":2,"kind":"Article","sha256":"2d94e0012de929dd2ecbcd527816ae7f308f614eceaccf0e07cd87545a95711a","slug":"freqai-reinforcement-learning","location":"/freqai-reinforcement-learning.md","dependencies":[],"frontmatter":{"title":"FreqAI 强化学习指南","description":"本文档详细介绍了 FreqAI 的强化学习功能,包括基本概念、运行方法、环境配置等内容。这些功能可以帮助用户构建和训练强化学习模型进行自动交易。","short_title":"强化学习","subtitle":"强化学习模型的详细说明","subject":"FreqAI 强化学习文档","authors":[{"id":"Freqtrade","name":"Freqtrade"}],"github":"https://github.com/freqtrade-cn/docs_zh-CN","keywords":["Freqtrade","中文文档","交易机器人","量化交易","加密货币","数字货币","区块链","人工智能","机器学习"],"numbering":{"title":{"offset":1}},"edit_url":"https://github.com/freqtrade-cn/docs_zh-CN/blob/main/freqai-reinforcement-learning.md","thumbnail":"/build/tensorboard-27f8850881840b007f5a56a202abb949.jpg","thumbnailOptimized":"/build/tensorboard-27f8850881840b007f5a56a202abb949.webp","exports":[{"format":"md","filename":"freqai-reinforcement-learning.md","url":"/build/freqai-reinforcement-cb72db54ba15f96c69ab38f94769520d.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"强化学习","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"oI392QVmRR"}],"identifier":"id","label":"强化学习","html_id":"id","implicit":true,"key":"dSsYRrOrPe"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"安装体积","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"fq1R1nSebJ"}],"key":"XyR2SGcFyK"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"强化学习依赖项包含如 ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"ctIkFsu2NS"},{"type":"inlineCode","value":"torch","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"c7Z5nICUFY"},{"type":"text","value":" 这样的大型包，需在执行 ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"OvzgwdMZTx"},{"type":"inlineCode","value":"./setup.sh -i","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"I6HU7YV8pa"},{"type":"text","value":" 时，在\"Do you also want dependencies for freqai-rl (~700mb additional space required) [y/N]?”问题上选择\"y\"以显式安装。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"NSAHZbXzNJ"}],"key":"lnKaOlG9wa"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"喜欢使用 docker 的用户应确保使用带有 ","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"qWDhIZvOMH"},{"type":"inlineCode","value":"_freqairl","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"TQ3gnLsA7A"},{"type":"text","value":" 后缀的 docker 镜像。","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"TAPGRXcHm2"}],"key":"M09rkxI0Gr"}],"key":"Fux3ljOmo3"},{"type":"heading","depth":3,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"背景与术语","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"paiMORQwiV"}],"identifier":"id","label":"背景与术语","html_id":"id-1","implicit":true,"key":"YsgWIc3FOW"},{"type":"heading","depth":4,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"什么是 RL，FreqAI 为什么需要它？","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"tM9Hh8Y96u"}],"identifier":"id-rl-freqai","label":"什么是 RL，FreqAI 为什么需要它？","html_id":"id-rl-freqai","implicit":true,"key":"ORGWasjymU"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"强化学习涉及两个重要组成部分：","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"CnqBbGkXMN"},{"type":"emphasis","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"智能体（agent）","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"sla9fOCyDi"},{"type":"emphasis","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"和训练","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"R4kUUqyRAS"}],"key":"xwVq800ZaU"},{"type":"text","value":"环境（environment）","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"P0RanOidqa"}],"key":"dYxyYOLP1p"},{"type":"text","value":"。在智能体训练期间，智能体逐根遍历历史蜡烛数据，每次做出一组动作中的一个：多头开仓、多头平仓、空头开仓、空头平仓、中立。在此训练过程中，环境会跟踪这些动作的表现，并根据自定义的 ","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"wEYnPCLufM"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"NsPZTLViEa"},{"type":"text","value":"（我们为用户提供了一个默认奖励函数，详情见","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"gGZNgw5fDI"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"gXBC9faebl"}],"urlSource":"#creating-a-custom-reward-function","key":"mbLEfSgo1v"},{"type":"text","value":"）对智能体进行奖励。奖励用于训练神经网络中的权重。","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"VJbhL13eHJ"}],"key":"n3Obk9xGCm"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"FreqAI RL 实现的另一个重要组成部分是*状态（state）*信息的使用。每一步都会将状态信息（如当前利润、当前持仓、当前交易持续时间）输入网络。这些信息用于训练环境中的智能体，并在 dry/live 时强化智能体（此功能在回测中不可用）。","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"vRPyaGgk0v"},{"type":"emphasis","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"FreqAI + Freqtrade 是这种强化机制的完美结合，因为这些信息在实时部署中随时可用。","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"SKuTac7SRG"}],"key":"B2EiPzj1Jh"}],"key":"zqeNZD6vt5"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"强化学习是 FreqAI 的自然进化，因为它为市场自适应和反应性增加了新的层次，这是分类器和回归器无法比拟的。然而，分类器和回归器也有 RL 不具备的优势，比如稳健的预测。训练不当的 RL 智能体可能会找到\"漏洞\"或\"技巧\"来最大化奖励，但实际上并未获得任何交易收益。因此，RL 更加复杂，需要比典型分类器和回归器更高的理解水平。","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"qFTgMXiSrE"}],"key":"olHUTnCWqV"},{"type":"heading","depth":4,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"RL 接口","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"Ka6U91z7y8"}],"identifier":"rl","label":"RL 接口","html_id":"rl","implicit":true,"key":"dbPu1Szjgm"},{"type":"paragraph","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"在当前框架下，我们旨在通过通用的\"预测模型\"文件暴露训练环境，该文件是用户继承的 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"pElzVxqw2A"},{"type":"inlineCode","value":"BaseReinforcementLearner","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"pstYY2oYEV"},{"type":"text","value":" 对象（如 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"bAl5ppOo6N"},{"type":"inlineCode","value":"freqai/prediction_models/ReinforcementLearner","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"JuGV3D43Ey"},{"type":"text","value":"）。在此用户类中，RL 环境可通过 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"Y6ffE5W1Kh"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"jrYS39xMlB"},{"type":"text","value":" 进行自定义（见","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"oWd363piuV"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"下文","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"RBnjMR8mBs"}],"urlSource":"#creating-a-custom-reward-function","key":"wN9TNxQobL"},{"type":"text","value":"）。","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"sBz8KX4TG6"}],"key":"Qk2mO3jMok"},{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"我们设想大多数用户会将精力集中在创造性设计 ","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"sEyevqpOCQ"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"WYDRNF6I6C"},{"type":"text","value":" 函数（详情见","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"DA23phx0vG"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"SC4aVSgiIN"}],"urlSource":"#creating-a-custom-reward-function","key":"CHOgk3JMlh"},{"type":"text","value":"），而对环境的其他部分保持不变。其他用户甚至不会修改环境，只会调整配置和 FreqAI 已有的强大特征工程。与此同时，我们也允许高级用户完全自定义自己的模型类。","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"dkYJoasMha"}],"key":"kDWaUNt0hl"},{"type":"paragraph","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"该框架基于 stable_baselines3（torch）和 OpenAI gym 构建基础环境类。但总体而言，模型类隔离良好，因此可以轻松集成其他竞争库。环境继承自 ","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"l1D2wMuAck"},{"type":"inlineCode","value":"gym.Env","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"ldEiyQ0mMT"},{"type":"text","value":"，因此如需切换到其他库，需编写全新的环境。","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"D94vCFi28I"}],"key":"WSD2QSHuhP"},{"type":"heading","depth":4,"position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"重要注意事项","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"ufJAhq8U7G"}],"identifier":"id","label":"重要注意事项","html_id":"id-2","implicit":true,"key":"gKeaLXba0M"},{"type":"paragraph","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"如上所述，智能体在一个\"人工\"交易\"环境\"中被\"训练\"。在我们的案例中，这个环境看起来与真实的 Freqtrade 回测环境很相似，但它","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"Yezp8DwN0i"},{"type":"emphasis","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"不是","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"HZsJVU5bWo"}],"key":"IAKGSfpFZJ"},{"type":"text","value":"。实际上，RL 训练环境要简单得多。它不包含任何复杂的策略逻辑，如 ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"eJTP6bT0vW"},{"type":"inlineCode","value":"custom_exit","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"LwOPDTb0qm"},{"type":"text","value":"、","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"HXtt1tHcNq"},{"type":"inlineCode","value":"custom_stoploss","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"ghiLmksyUb"},{"type":"text","value":"、杠杆控制等回调。RL 环境是对真实市场的非常\"原始\"表示，智能体可以自由学习由 ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"yrkbbjFhSt"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"uQ1tauWnci"},{"type":"text","value":" 强制执行的策略（如止损、止盈等）。因此，必须注意，智能体训练环境并不等同于真实世界。","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"MiKIWwncgH"}],"key":"gWlHZ37chJ"},{"type":"heading","depth":3,"position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"运行强化学习","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"F8y9rCAO3x"}],"identifier":"id","label":"运行强化学习","html_id":"id-3","implicit":true,"key":"cZ09txTAG8"},{"type":"paragraph","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"text","value":"设置和运行强化学习模型与运行回归器或分类器相同。命令行上必须定义同样的两个参数：","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"YCZcvkz1iI"},{"type":"inlineCode","value":"--freqaimodel","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"oFQ5onJkjg"},{"type":"text","value":" 和 ","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"UqEOdGIivS"},{"type":"inlineCode","value":"--strategy","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"P1RTLxiYDZ"},{"type":"text","value":"：","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"fzSEPPfrqD"}],"key":"ZJR4gVdANz"},{"type":"code","lang":"bash","value":"freqtrade trade --freqaimodel ReinforcementLearner --strategy MyRLStrategy --config config.json","position":{"start":{"line":43,"column":1},"end":{"line":45,"column":1}},"key":"BF3jvwaA8w"},{"type":"paragraph","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"其中 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"izkd7FN8fw"},{"type":"inlineCode","value":"ReinforcementLearner","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"eWd5OqETOk"},{"type":"text","value":" 将使用 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"HYl9lXUiUr"},{"type":"inlineCode","value":"freqai/prediction_models/ReinforcementLearner","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"CCvRf5TN6a"},{"type":"text","value":" 中的模板类（或 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"VL3MTgGu36"},{"type":"inlineCode","value":"user_data/freqaimodels","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"S2oarHstZ9"},{"type":"text","value":" 下的自定义类）。策略部分则与典型回归器一样，采用","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"rtdcVoyw1r"},{"type":"link","url":"/freqai-feature-engineering","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"特征工程","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"JfQAjj0OgO"}],"urlSource":"freqai-feature-engineering.md","dataUrl":"/freqai-feature-engineering.json","internal":true,"protocol":"file","key":"B2bl2SqcPb"},{"type":"text","value":"的 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"eR5OpFi4xK"},{"type":"inlineCode","value":"feature_engineering_*","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"vkD7989Gtv"},{"type":"text","value":"。不同之处在于目标的创建，强化学习不需要目标标签。然而，FreqAI 要求在动作列中设置一个默认（中立）值：","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"w8sliyasVb"}],"key":"pZ7X064kyN"},{"type":"code","lang":"python","value":"    def set_freqai_targets(self, dataframe, **kwargs) -> DataFrame:\n        \"\"\"\n        *仅适用于启用 FreqAI 的策略*\n        设置模型目标的必需函数。\n        所有目标必须以 `&` 开头，以便 FreqAI 内部识别。\n\n        更多特征工程细节见：\n\n        https://www.freqtrade.io/en/latest/freqai-feature-engineering\n\n        :param df: 将接收目标的策略数据框\n        使用示例：dataframe[\"&-target\"] = dataframe[\"close\"].shift(-1) / dataframe[\"close\"]\n        \"\"\"\n        # 对于 RL，无需设置直接目标。此处为占位（中立），直到智能体发送动作。\n        dataframe[\"&-action\"] = 0\n        return dataframe","position":{"start":{"line":49,"column":1},"end":{"line":66,"column":1}},"key":"SCu3cYzwxQ"},{"type":"paragraph","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"children":[{"type":"text","value":"大部分函数与典型回归器相同，但下方函数展示了策略如何将原始价格数据传递给智能体，以便其在训练环境中访问原始 OHLCV：","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"i7lq5ddpOm"}],"key":"MS8wkWxUfV"},{"type":"code","lang":"python","value":"    def feature_engineering_standard(self, dataframe: DataFrame, **kwargs) -> DataFrame:\n        # 以下特征对 RL 模型是必要的\n        dataframe[f\"%-raw_close\"] = dataframe[\"close\"]\n        dataframe[f\"%-raw_open\"] = dataframe[\"open\"]\n        dataframe[f\"%-raw_high\"] = dataframe[\"high\"]\n        dataframe[f\"%-raw_low\"] = dataframe[\"low\"]\n    return dataframe","position":{"start":{"line":70,"column":1},"end":{"line":78,"column":1}},"key":"gRgyCFzBjk"},{"type":"paragraph","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"children":[{"type":"text","value":"最后，没有显式的\"标签\"需要设置——而是需要分配 ","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"ajWfflKLnK"},{"type":"inlineCode","value":"&-action","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"KXvzUIceOV"},{"type":"text","value":" 列，该列在 ","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"OsgFeL5a1G"},{"type":"inlineCode","value":"populate_entry/exit_trends()","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"JX2BXTrUsv"},{"type":"text","value":" 中被访问时包含智能体的动作。在本例中，中立动作为 0。此值应与所用环境一致。FreqAI 提供的两个环境均以 0 作为中立动作。","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"P9PgypJZ5i"}],"key":"zKc41Q02Il"},{"type":"paragraph","position":{"start":{"line":82,"column":1},"end":{"line":82,"column":1}},"children":[{"type":"text","value":"用户会很快意识到无需设置标签，智能体会\"自主\"做出进出场决策。这使得策略构建变得相当简单。进出场信号由智能体以整数形式给出，直接用于策略中的进出场判断：","position":{"start":{"line":82,"column":1},"end":{"line":82,"column":1}},"key":"IhASWGaeom"}],"key":"uBmpaBVj6O"},{"type":"code","lang":"python","value":"    def populate_entry_trend(self, df: DataFrame, metadata: dict) -> DataFrame:\n\n        enter_long_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 1]\n\n        if enter_long_conditions:\n            df.loc[\n                reduce(lambda x, y: x & y, enter_long_conditions), [\"enter_long\", \"enter_tag\"]\n            ] = (1, \"long\")\n\n        enter_short_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 3]\n\n        if enter_short_conditions:\n            df.loc[\n                reduce(lambda x, y: x & y, enter_short_conditions), [\"enter_short\", \"enter_tag\"]\n            ] = (1, \"short\")\n\n        return df\n\n    def populate_exit_trend(self, df: DataFrame, metadata: dict) -> DataFrame:\n        exit_long_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 2]\n        if exit_long_conditions:\n            df.loc[reduce(lambda x, y: x & y, exit_long_conditions), \"exit_long\"] = 1\n\n        exit_short_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 4]\n        if exit_short_conditions:\n            df.loc[reduce(lambda x, y: x & y, exit_short_conditions), \"exit_short\"] = 1\n\n        return df","position":{"start":{"line":84,"column":1},"end":{"line":113,"column":1}},"key":"BogPCBB1ZO"},{"type":"paragraph","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"children":[{"type":"text","value":"需要注意的是，","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"h3NvhU4sSp"},{"type":"inlineCode","value":"&-action","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"nfzaQEeYqT"},{"type":"text","value":" 取决于所选环境。上述示例展示了 5 个动作，其中 0 为中立，1 为多头开仓，2 为多头平仓，3 为空头开仓，4 为空头平仓。","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"ItwVP8ZCiY"}],"key":"ts0KvCu7wf"},{"type":"heading","depth":3,"position":{"start":{"line":117,"column":1},"end":{"line":117,"column":1}},"children":[{"type":"text","value":"配置强化学习器","position":{"start":{"line":117,"column":1},"end":{"line":117,"column":1}},"key":"zQguTBkSKo"}],"identifier":"id","label":"配置强化学习器","html_id":"id-4","implicit":true,"key":"gUBoz8CSvD"},{"type":"paragraph","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"children":[{"type":"text","value":"要配置 ","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"sYSuE2QwI9"},{"type":"inlineCode","value":"Reinforcement Learner","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"CZdUxVijIK"},{"type":"text","value":"，需在 ","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"SRDXkHseY6"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"tDFXFlLVFZ"},{"type":"text","value":" 配置中包含如下字典：","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"GJ52byreJ4"}],"key":"EtvqIY25Ti"},{"type":"code","lang":"json","value":"        \"rl_config\": {\n            \"train_cycles\": 25,\n            \"add_state_info\": true,\n            \"max_trade_duration_candles\": 300,\n            \"max_training_drawdown_pct\": 0.02,\n            \"cpu_count\": 8,\n            \"model_type\": \"PPO\",\n            \"policy_type\": \"MlpPolicy\",\n            \"model_reward_parameters\": {\n                \"rr\": 1,\n                \"profit_aim\": 0.025\n            }\n        }","position":{"start":{"line":121,"column":1},"end":{"line":135,"column":1}},"key":"eeZYDqVQDk"},{"type":"paragraph","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"参数详情见","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"xf6fAHE8nT"},{"type":"link","url":"/freqai-parameter-table","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"参数表","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"lMSPf0FW8b"}],"urlSource":"freqai-parameter-table.md","dataUrl":"/freqai-parameter-table.json","internal":true,"protocol":"file","key":"GHeYHw3Awh"},{"type":"text","value":"。一般来说，","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"o1G9ZM4qWD"},{"type":"inlineCode","value":"train_cycles","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"Sww879uSsT"},{"type":"text","value":" 决定智能体在其人工环境中遍历蜡烛数据以训练模型权重的次数。","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"WkiOivB4T6"},{"type":"inlineCode","value":"model_type","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"OEWO5JZxbl"},{"type":"text","value":" 是一个字符串，选择 ","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"naGRPpnxAT"},{"type":"link","url":"https://stable-baselines3.readthedocs.io/en/master/","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"stable_baselines","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"N0VkapBmaE"}],"urlSource":"https://stable-baselines3.readthedocs.io/en/master/","key":"wfJG2TXjHL"},{"type":"text","value":"（外部链接）中可用的模型之一。","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"UwMj0Wrbdg"}],"key":"XLgQOOS940"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"M3sRnxOIUs"}],"key":"CsDrsoFDsx"},{"type":"paragraph","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"children":[{"type":"text","value":"如果你想尝试 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"qt5ucX6rK6"},{"type":"inlineCode","value":"continual_learning","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"xDRdsDOifT"},{"type":"text","value":"，应在主 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"Yye4bAgB9G"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"VTgaaX6PdR"},{"type":"text","value":" 配置字典中将其设为 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"ZNbuKFQPa0"},{"type":"inlineCode","value":"true","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"Apy3dKnkDk"},{"type":"text","value":"。这会让强化学习库在每次再训练时，从前一模型的最终状态继续训练新模型，而不是每次都从头训练。","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"kKJe7VRwJS"}],"key":"R7xZ0W0Uiy"}],"key":"eOokGj7RYJ"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"h5Bgdw61dV"}],"key":"MEI9QFJch4"},{"type":"paragraph","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"children":[{"type":"text","value":"请记住，通用的 ","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"qKC9zqFVCI"},{"type":"inlineCode","value":"model_training_parameters","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"stvlJFH1ol"},{"type":"text","value":" 字典应包含特定 ","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"NRUY0nDjss"},{"type":"inlineCode","value":"model_type","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"KiMOYa1A4j"},{"type":"text","value":" 的所有模型超参数。例如，","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"kd3fFrHGEa"},{"type":"inlineCode","value":"PPO","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"rBFxXvpBXy"},{"type":"text","value":" 参数见","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"Aq9q4iZyan"},{"type":"link","url":"https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"Wu5ny5cAzn"}],"urlSource":"https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html","key":"Gz3YrZrDOL"},{"type":"text","value":"。","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"iqAv2zD4p9"}],"key":"R1UOzVAyAA"}],"key":"Tj0GuMP7rT"},{"type":"heading","depth":3,"position":{"start":{"line":147,"column":1},"end":{"line":147,"column":1}},"children":[{"type":"text","value":"创建自定义奖励函数","position":{"start":{"line":147,"column":1},"end":{"line":147,"column":1}},"key":"g4lteUJuhC"}],"identifier":"id","label":"创建自定义奖励函数","html_id":"id-5","implicit":true,"key":"bDF4IHeKhe"},{"type":"admonition","kind":"danger","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"不适用于生产环境","position":{"start":{"line":149,"column":1},"end":{"line":149,"column":1}},"key":"u6MNypoMUb"}],"key":"PxOm73wzSb"},{"type":"paragraph","position":{"start":{"line":150,"column":1},"end":{"line":150,"column":1}},"children":[{"type":"text","value":"警告！","position":{"start":{"line":150,"column":1},"end":{"line":150,"column":1}},"key":"g64dnz3RaR"}],"key":"QddPgKY2Wu"},{"type":"paragraph","position":{"start":{"line":152,"column":1},"end":{"line":152,"column":1}},"children":[{"type":"text","value":"Freqtrade 源码中提供的奖励函数仅用于展示功能，旨在展示/测试尽可能多的环境控制特性，并在小型计算机上快速运行。它是基准，不适用于生产环境。请注意，你需要自己创建 custom_reward() 函数，或使用其他用户在 Freqtrade 源码之外构建的模板。","position":{"start":{"line":152,"column":1},"end":{"line":152,"column":1}},"key":"nQ3Pkil6bw"}],"key":"w731B7NfUn"}],"key":"V9u86OoSZU"},{"type":"paragraph","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"当你开始修改策略和预测模型时，会很快发现强化学习器与回归器/分类器有一些重要区别。首先，策略不设置目标值（无标签！）。相反，你需要在 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"DFQyv0nEAO"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"PuzgTPdaGd"},{"type":"text","value":" 类中设置 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"taABR5GnDQ"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"daFnpEICNC"},{"type":"text","value":" 函数（见下文）。","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"TS1Vhrorxf"},{"type":"inlineCode","value":"prediction_models/ReinforcementLearner.py","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"FC6DwJzS1C"},{"type":"text","value":" 中提供了一个默认的 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"x9CeTaqAmq"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"kg1VYacLSM"},{"type":"text","value":"，用于演示奖励构建的必要模块，但","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"Y5qhyTNR6X"},{"type":"emphasis","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"不适用于生产","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"AJIcLoHnr8"}],"key":"maRpU8NRrl"},{"type":"text","value":"。用户","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"JkAMKTgQDo"},{"type":"emphasis","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"必须","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"ZxE6Ca6Qmk"}],"key":"ZhBXkMRqjO"},{"type":"text","value":"创建自己的自定义强化学习模型类，或使用 Freqtrade 源码之外的预构建模型并保存到 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"K2ERENbq8L"},{"type":"inlineCode","value":"user_data/freqaimodels","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"GB9FBy7IrG"},{"type":"text","value":"。在 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"O42tyV1Obe"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"Bg8bsSyDqO"},{"type":"text","value":" 中可以表达你对市场的创造性理论。例如，你可以在智能体获利时奖励它，亏损时惩罚它，或奖励其开仓、惩罚其持仓过久。下方展示了这些奖励的计算方式：","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"FYj7jxUD3T"}],"key":"BqWvFgE3RL"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"提示","position":{"start":{"line":157,"column":1},"end":{"line":157,"column":1}},"key":"d2vKe8saQj"}],"key":"h0hZrNdzcT"},{"type":"paragraph","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"children":[{"type":"text","value":"最好的奖励函数是连续可微且缩放良好的。换句话说，对罕见事件施加一次性大负惩罚不是好主意，神经网络无法学会这种函数。更好的做法是对常见事件施加小负惩罚，这有助于智能体更快学习。你还可以通过让奖励/惩罚随某些线性/指数函数按严重程度缩放来提升连续性。例如，随着持仓时间增加，逐步增加惩罚，这比在某一时刻一次性施加大惩罚更好。","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"key":"xsESeSKHCT"}],"key":"nc7weWzrty"}],"key":"ZubHjyS9lS"},{"type":"code","lang":"python","value":"from freqtrade.freqai.prediction_models.ReinforcementLearner import ReinforcementLearner\nfrom freqtrade.freqai.RL.Base5ActionRLEnv import Actions, Base5ActionRLEnv, Positions\n\n\nclass MyCoolRLModel(ReinforcementLearner):\n    \"\"\"\n    用户自定义 RL 预测模型。\n\n    将此文件保存到 `freqtrade/user_data/freqaimodels`\n\n    然后用如下命令调用：\n\n    freqtrade trade --freqaimodel MyCoolRLModel --config config.json --strategy SomeCoolStrat\n\n    用户可重写 `IFreqaiModel` 继承树中的任意函数。对 RL 来说，最重要的是在此重写 `MyRLEnv`（见下文），以定义自定义 `calculate_reward()`，或重写环境的其他部分。\n\n    此类还允许用户重写 IFreqaiModel 树的其他部分。例如，可以重写 `def fit()`、`def train()` 或 `def predict()` 以精细控制这些过程。\n\n    另一个常见重写是 `def data_cleaning_predict()`，用于精细控制数据处理管道。\n    \"\"\"\n    class MyRLEnv(Base5ActionRLEnv):\n        \"\"\"\n        用户自定义环境。此类继承自 BaseEnvironment 和 gym.Env。\n        用户可重写父类的任意函数。以下为自定义 `calculate_reward()` 的示例。\n\n        警告！\n        此函数仅用于展示功能，旨在展示尽可能多的环境控制特性，并在小型计算机上快速运行。它是基准，不适用于生产环境。\n        \"\"\"\n        def calculate_reward(self, action: int) -> float:\n            # 首先，若动作无效则惩罚\n            if not self._is_valid(action):\n                return -2\n            pnl = self.get_unrealized_profit()\n\n            factor = 100\n\n            pair = self.pair.replace(':', '')\n\n            # 可使用 dataframe 中的特征值\n            # 假设策略中已生成移位的 RSI 指标。\n            rsi_now = self.raw_features[f\"%-rsi-period_10_shift-1_{pair}_\"\n                            f\"{self.config['timeframe']}\"]\\\n                            .iloc[self._current_tick]\n\n            # 奖励智能体开仓\n            if (action in (Actions.Long_enter.value, Actions.Short_enter.value)\n                    and self._position == Positions.Neutral):\n                if rsi_now < 40:\n                    factor = 40 / rsi_now\n                else:\n                    factor = 1\n                return 25 * factor\n\n            # 惩罚智能体未开仓\n            if action == Actions.Neutral.value and self._position == Positions.Neutral:\n                return -1\n            max_trade_duration = self.rl_config.get('max_trade_duration_candles', 300)\n            trade_duration = self._current_tick - self._last_trade_tick\n            if trade_duration <= max_trade_duration:\n                factor *= 1.5\n            elif trade_duration > max_trade_duration:\n                factor *= 0.5\n            # 惩罚持仓不动\n            if self._position in (Positions.Short, Positions.Long) and \\\n            action == Actions.Neutral.value:\n                return -1 * trade_duration / max_trade_duration\n            # 多头平仓\n            if action == Actions.Long_exit.value and self._position == Positions.Long:\n                if pnl > self.profit_aim * self.rr:\n                    factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n                return float(pnl * factor)\n            # 空头平仓\n            if action == Actions.Short_exit.value and self._position == Positions.Short:\n                if pnl > self.profit_aim * self.rr:\n                    factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n                return float(pnl * factor)\n            return 0.","position":{"start":{"line":161,"column":1},"end":{"line":239,"column":1}},"key":"KqGRTwR2Gt"},{"type":"heading","depth":3,"position":{"start":{"line":241,"column":1},"end":{"line":241,"column":1}},"children":[{"type":"text","value":"使用 Tensorboard","position":{"start":{"line":241,"column":1},"end":{"line":241,"column":1}},"key":"Lrnp3Hgxs1"}],"identifier":"id-tensorboard","label":"使用 Tensorboard","html_id":"id-tensorboard","implicit":true,"key":"FmwIepde82"},{"type":"paragraph","position":{"start":{"line":243,"column":1},"end":{"line":243,"column":1}},"children":[{"type":"text","value":"强化学习模型受益于训练指标的跟踪。FreqAI 集成了 Tensorboard，允许用户跨所有币种和所有再训练过程跟踪训练和评估表现。可通过以下命令激活 Tensorboard：","position":{"start":{"line":243,"column":1},"end":{"line":243,"column":1}},"key":"eAtAwVsG5e"}],"key":"eRRzKTbMqj"},{"type":"code","lang":"bash","value":"tensorboard --logdir user_data/models/unique-id","position":{"start":{"line":245,"column":1},"end":{"line":247,"column":1}},"key":"qSmmMddU2F"},{"type":"paragraph","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"children":[{"type":"text","value":"其中 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"kCoyXhJ2lJ"},{"type":"inlineCode","value":"unique-id","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"S5ylcRSaAE"},{"type":"text","value":" 是 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"CDnCspBf0l"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"Rsd0MtTB0T"},{"type":"text","value":" 配置文件中设置的 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"JgvNNAHZNZ"},{"type":"inlineCode","value":"identifier","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"Sj9nTqkUG0"},{"type":"text","value":"。该命令需在单独的 shell 中运行，浏览器访问 127.0.0.1:6006（6006 为 Tensorboard 默认端口）查看输出。","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"KjxAFHW2fa"}],"key":"P1sJWRyc5W"},{"type":"image","url":"/build/tensorboard-27f8850881840b007f5a56a202abb949.jpg","alt":"tensorboard","position":{"start":{"line":251,"column":1},"end":{"line":251,"column":1}},"key":"kNGIheESs4","urlSource":"assets/tensorboard.jpg","urlOptimized":"/build/tensorboard-27f8850881840b007f5a56a202abb949.webp"},{"type":"heading","depth":3,"position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"children":[{"type":"text","value":"自定义日志","position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"key":"cp9ChKDT6Z"}],"identifier":"id","label":"自定义日志","html_id":"id-6","implicit":true,"key":"PIdbu6N1Fq"},{"type":"paragraph","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"children":[{"type":"text","value":"FreqAI 还内置了一个名为 ","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"YaStVKMXzf"},{"type":"inlineCode","value":"self.tensorboard_log","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"Xx15STwd39"},{"type":"text","value":" 的分集摘要日志器，用于将自定义信息添加到 Tensorboard 日志。默认情况下，该函数在环境内每步调用一次以记录智能体动作。单集内所有步的值在每集结束时汇总报告，然后所有指标重置为 0，为下一个 episode 做准备。","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"HLoP0nv76q"}],"key":"gBTCSJBJAP"},{"type":"paragraph","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"children":[{"type":"inlineCode","value":"self.tensorboard_log","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"Rio8UN7dcV"},{"type":"text","value":" 也可在环境内任意位置使用，例如可在 ","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"BVpcitgvCW"},{"type":"inlineCode","value":"calculate_reward","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"rcJsJOq7WF"},{"type":"text","value":" 函数中添加，以收集奖励各部分被调用的频率：","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"kSI1oTXROt"}],"key":"g1s7PsIJAG"},{"type":"code","lang":"python","value":"    class MyRLEnv(Base5ActionRLEnv):\n        \"\"\"\n        用户自定义环境。此类继承自 BaseEnvironment 和 gym.Env。\n        用户可重写父类的任意函数。以下为自定义 `calculate_reward()` 的示例。\n        \"\"\"\n        def calculate_reward(self, action: int) -> float:\n            if not self._is_valid(action):\n                self.tensorboard_log(\"invalid\")\n                return -2\n","position":{"start":{"line":259,"column":1},"end":{"line":270,"column":1}},"key":"IrfGtUZDcO"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"ip4XQbsi0J"}],"key":"zMLlneb6CK"},{"type":"paragraph","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"children":[{"type":"inlineCode","value":"self.tensorboard_log()","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"aTRtuv9vGO"},{"type":"text","value":" 仅用于跟踪计数对象（如事件、动作）。如需记录浮点数，可作为第二参数传入，如 ","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"bWRowOxLUc"},{"type":"inlineCode","value":"self.tensorboard_log(\"float_metric1\", 0.23)","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"j1poctRdjM"},{"type":"text","value":"。此时指标值不会累加。","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"WFfFHxcSpY"}],"key":"pLiMuzG5Lr"}],"key":"Sh7KLGYlft"},{"type":"heading","depth":3,"position":{"start":{"line":276,"column":1},"end":{"line":276,"column":1}},"children":[{"type":"text","value":"选择基础环境","position":{"start":{"line":276,"column":1},"end":{"line":276,"column":1}},"key":"uHHIoa2BMi"}],"identifier":"id","label":"选择基础环境","html_id":"id-7","implicit":true,"key":"Tcm7Yc7U56"},{"type":"paragraph","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"children":[{"type":"text","value":"FreqAI 提供三种基础环境：","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"rs1Kp31qPz"},{"type":"inlineCode","value":"Base3ActionRLEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"ouH4skcrea"},{"type":"text","value":"、","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"uBISe56wXf"},{"type":"inlineCode","value":"Base4ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"fgcjsm6jpM"},{"type":"text","value":" 和 ","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"pVSY5REp5c"},{"type":"inlineCode","value":"Base5ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"WaoX1lw5vg"},{"type":"text","value":"。顾名思义，这些环境分别适用于可选择 3、4、5 种动作的智能体。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"E1bSFpPmnc"},{"type":"inlineCode","value":"Base3ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"qeHcT8BGvg"},{"type":"text","value":" 最简单，智能体可选择持有、多头或空头。该环境也可用于仅做多的机器人（自动跟随策略的 ","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"Mm9Iw9nSJU"},{"type":"inlineCode","value":"can_short","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"Q5OgDMrlnm"},{"type":"text","value":" 标志），其中多头为开仓，空头为平仓。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"LkMq2IuNbz"},{"type":"inlineCode","value":"Base4ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"FSlyDPvC3W"},{"type":"text","value":" 中，智能体可多头开仓、空头开仓、中立持有或平仓。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"Ft6pSMoocg"},{"type":"inlineCode","value":"Base5ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"k0Npte5K7d"},{"type":"text","value":" 则与 Base4 类似，但将平仓动作区分为多头平仓和空头平仓。环境选择的主要影响包括：","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"OerM2esfko"}],"key":"N0Oyahcs4f"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":280,"column":1},"end":{"line":282,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"children":[{"type":"inlineCode","value":"calculate_reward","position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"key":"TjVDpy2z1X"},{"type":"text","value":" 中可用的动作","position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"key":"nUM6IcmKBT"}],"key":"Xu0ZsWW5sO"},{"type":"listItem","spread":true,"position":{"start":{"line":281,"column":1},"end":{"line":282,"column":1}},"children":[{"type":"text","value":"用户策略中消费的动作","position":{"start":{"line":281,"column":1},"end":{"line":281,"column":1}},"key":"LgkX6FbaaO"}],"key":"akGrQmNf8o"}],"key":"OE1LifUrYj"},{"type":"paragraph","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"children":[{"type":"text","value":"FreqAI 提供的所有环境均继承自动作/持仓无关的 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"Qj7k8OgEjd"},{"type":"inlineCode","value":"BaseEnvironment","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"JND2ZQIhXE"},{"type":"text","value":"，该对象包含所有共享逻辑。架构设计易于自定义。最简单的自定义是 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"r1EqHsmp10"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"qY02uLRQeT"},{"type":"text","value":"（详见","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"FnjpTrVD0r"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"R0FunDufOr"}],"urlSource":"#creating-a-custom-reward-function","key":"TrQWYf8uxo"},{"type":"text","value":"）。当然，也可进一步扩展环境内的任意函数，只需在预测模型文件的 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"Gt9YGcs1bs"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"aEyJE56bWd"},{"type":"text","value":" 中重写这些函数即可。更高级的自定义建议直接继承 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"aIqPHyZnvq"},{"type":"inlineCode","value":"BaseEnvironment","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"g2yMT7GXVv"},{"type":"text","value":" 创建全新环境。","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"BnH1HZf0AO"}],"key":"cfKLMfXflJ"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"zlukI0JfRl"}],"key":"Cp3XmORDW4"},{"type":"paragraph","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"children":[{"type":"text","value":"仅 ","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"o7AAOzVaKT"},{"type":"inlineCode","value":"Base3ActionRLEnv","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"cNvGMDa4XY"},{"type":"text","value":" 支持仅做多训练/交易（将用户策略属性 ","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"XwWmNhJHYa"},{"type":"inlineCode","value":"can_short = False","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"cGTdKA7NIJ"},{"type":"text","value":"）。","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"ktYTvJx9P3"}],"key":"nBYv20GRbU"}],"key":"hZg9vkjVyK"}],"key":"UBUR0fT8wz"}],"key":"BzjOuhfF9A"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"FreqAI 运行指南","short_title":"FreqAI 运行","url":"/freqai-running","group":"FreqAI"},"next":{"title":"FreqAI 开发者指南","short_title":"FreqAI 开发","url":"/freqai-developers","group":"FreqAI"}}},"domain":"http://localhost:3001"}