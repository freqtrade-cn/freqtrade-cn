{"version":2,"kind":"Article","sha256":"2d94e0012de929dd2ecbcd527816ae7f308f614eceaccf0e07cd87545a95711a","slug":"freqai-reinforcement-learning","location":"/freqai-reinforcement-learning.md","dependencies":[],"frontmatter":{"title":"FreqAI 强化学习指南","description":"本文档详细介绍了 FreqAI 的强化学习功能,包括基本概念、运行方法、环境配置等内容。这些功能可以帮助用户构建和训练强化学习模型进行自动交易。","short_title":"强化学习","subtitle":"强化学习模型的详细说明","subject":"FreqAI 强化学习文档","authors":[{"id":"Freqtrade","name":"Freqtrade"}],"github":"https://github.com/freqtrade-cn/docs_zh-CN","keywords":["Freqtrade","中文文档","交易机器人","量化交易","加密货币","数字货币","区块链","人工智能","机器学习"],"numbering":{"title":{"offset":1}},"edit_url":"https://github.com/freqtrade-cn/docs_zh-CN/blob/main/freqai-reinforcement-learning.md","thumbnail":"/build/tensorboard-27f8850881840b007f5a56a202abb949.jpg","thumbnailOptimized":"/build/tensorboard-27f8850881840b007f5a56a202abb949.webp","exports":[{"format":"md","filename":"freqai-reinforcement-learning.md","url":"/build/freqai-reinforcement-cb72db54ba15f96c69ab38f94769520d.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"强化学习","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"b49Li53jdz"}],"identifier":"id","label":"强化学习","html_id":"id","implicit":true,"key":"b6E6EPprFr"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"安装体积","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"b65Ibfy6Pk"}],"key":"LL5bRoWGPv"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"强化学习依赖项包含如 ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"ehzLiNDmbi"},{"type":"inlineCode","value":"torch","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"v7AQg3yGO3"},{"type":"text","value":" 这样的大型包，需在执行 ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"sJGk3M6xio"},{"type":"inlineCode","value":"./setup.sh -i","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"a4ibZ0JSwN"},{"type":"text","value":" 时，在\"Do you also want dependencies for freqai-rl (~700mb additional space required) [y/N]?”问题上选择\"y\"以显式安装。","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"gWAC4Z9tYm"}],"key":"uoulOOAlvl"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"喜欢使用 docker 的用户应确保使用带有 ","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"jRCRHs4NeU"},{"type":"inlineCode","value":"_freqairl","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"jpN4N2xcfp"},{"type":"text","value":" 后缀的 docker 镜像。","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"C8XQ9sigmf"}],"key":"cV3PQCpYUy"}],"key":"uThcdD1jhn"},{"type":"heading","depth":3,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"背景与术语","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"oj6naBlbIz"}],"identifier":"id","label":"背景与术语","html_id":"id-1","implicit":true,"key":"lHF3W8ESoZ"},{"type":"heading","depth":4,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"什么是 RL，FreqAI 为什么需要它？","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"KpbhQCsJlr"}],"identifier":"id-rl-freqai","label":"什么是 RL，FreqAI 为什么需要它？","html_id":"id-rl-freqai","implicit":true,"key":"ySyIxFO786"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"强化学习涉及两个重要组成部分：","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"bkk88pAc05"},{"type":"emphasis","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"智能体（agent）","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"nNjJ04H2Et"},{"type":"emphasis","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"和训练","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"Nx7qCVxKdR"}],"key":"Uct3D1fnjW"},{"type":"text","value":"环境（environment）","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"lBzWjgsgyT"}],"key":"WiAFc4kvqN"},{"type":"text","value":"。在智能体训练期间，智能体逐根遍历历史蜡烛数据，每次做出一组动作中的一个：多头开仓、多头平仓、空头开仓、空头平仓、中立。在此训练过程中，环境会跟踪这些动作的表现，并根据自定义的 ","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"hTnhkWuhr4"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"S8KCxl1Fas"},{"type":"text","value":"（我们为用户提供了一个默认奖励函数，详情见","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"zk8jXmCcx7"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"ZjM648ofSZ"}],"urlSource":"#creating-a-custom-reward-function","key":"fSsIOMXjjT"},{"type":"text","value":"）对智能体进行奖励。奖励用于训练神经网络中的权重。","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"t8pIRDAAFY"}],"key":"BuuYhcZ6pE"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"FreqAI RL 实现的另一个重要组成部分是*状态（state）*信息的使用。每一步都会将状态信息（如当前利润、当前持仓、当前交易持续时间）输入网络。这些信息用于训练环境中的智能体，并在 dry/live 时强化智能体（此功能在回测中不可用）。","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"nWSDeLsGub"},{"type":"emphasis","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"FreqAI + Freqtrade 是这种强化机制的完美结合，因为这些信息在实时部署中随时可用。","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"EAYAKNQmEZ"}],"key":"SNPeksSl90"}],"key":"S6B6CbnhRC"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"强化学习是 FreqAI 的自然进化，因为它为市场自适应和反应性增加了新的层次，这是分类器和回归器无法比拟的。然而，分类器和回归器也有 RL 不具备的优势，比如稳健的预测。训练不当的 RL 智能体可能会找到\"漏洞\"或\"技巧\"来最大化奖励，但实际上并未获得任何交易收益。因此，RL 更加复杂，需要比典型分类器和回归器更高的理解水平。","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"u9GmwaHHna"}],"key":"ZqY7rT9MyJ"},{"type":"heading","depth":4,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"RL 接口","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"O75Alx2N6w"}],"identifier":"rl","label":"RL 接口","html_id":"rl","implicit":true,"key":"nLdhhexJgF"},{"type":"paragraph","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"在当前框架下，我们旨在通过通用的\"预测模型\"文件暴露训练环境，该文件是用户继承的 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"bTeDEnEJUf"},{"type":"inlineCode","value":"BaseReinforcementLearner","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"B6vltaeetP"},{"type":"text","value":" 对象（如 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"HpXfM6hQsH"},{"type":"inlineCode","value":"freqai/prediction_models/ReinforcementLearner","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"M42R2EZPk9"},{"type":"text","value":"）。在此用户类中，RL 环境可通过 ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"j1HPiFGh7Z"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"phyuhuBC6m"},{"type":"text","value":" 进行自定义（见","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"NEEedYZfc6"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"下文","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"XeYUakvQaG"}],"urlSource":"#creating-a-custom-reward-function","key":"JYjhFLyEvP"},{"type":"text","value":"）。","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"QfS6ubEu0k"}],"key":"U3u8fNPVi2"},{"type":"paragraph","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"我们设想大多数用户会将精力集中在创造性设计 ","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"LNll9iQtqp"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"cmIKYdkwnS"},{"type":"text","value":" 函数（详情见","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"xfJXWSshNu"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"fE4F8qcaFk"}],"urlSource":"#creating-a-custom-reward-function","key":"wHi7KFbtQR"},{"type":"text","value":"），而对环境的其他部分保持不变。其他用户甚至不会修改环境，只会调整配置和 FreqAI 已有的强大特征工程。与此同时，我们也允许高级用户完全自定义自己的模型类。","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"eJisHgLbRA"}],"key":"j3n4e0tyOh"},{"type":"paragraph","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"该框架基于 stable_baselines3（torch）和 OpenAI gym 构建基础环境类。但总体而言，模型类隔离良好，因此可以轻松集成其他竞争库。环境继承自 ","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"n66aGbG1Ze"},{"type":"inlineCode","value":"gym.Env","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"sHNyFLbvB5"},{"type":"text","value":"，因此如需切换到其他库，需编写全新的环境。","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"A8b46b7feS"}],"key":"QUxppI0fb1"},{"type":"heading","depth":4,"position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"重要注意事项","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"coLD71mr6d"}],"identifier":"id","label":"重要注意事项","html_id":"id-2","implicit":true,"key":"CtSwtgIXTW"},{"type":"paragraph","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"如上所述，智能体在一个\"人工\"交易\"环境\"中被\"训练\"。在我们的案例中，这个环境看起来与真实的 Freqtrade 回测环境很相似，但它","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"enCZrY8EQG"},{"type":"emphasis","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"不是","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"WcnUmmBAHG"}],"key":"o8Tx1U2n1L"},{"type":"text","value":"。实际上，RL 训练环境要简单得多。它不包含任何复杂的策略逻辑，如 ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"GGCmuiwQoq"},{"type":"inlineCode","value":"custom_exit","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"KE4EqnU9GR"},{"type":"text","value":"、","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"kafzSMJamK"},{"type":"inlineCode","value":"custom_stoploss","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"u91rKrg2dQ"},{"type":"text","value":"、杠杆控制等回调。RL 环境是对真实市场的非常\"原始\"表示，智能体可以自由学习由 ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"a2hEGpvuhG"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"NNsNFUF3Y7"},{"type":"text","value":" 强制执行的策略（如止损、止盈等）。因此，必须注意，智能体训练环境并不等同于真实世界。","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"CMUKald6fk"}],"key":"ANqpGovB7u"},{"type":"heading","depth":3,"position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"运行强化学习","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"FkkAg1Ps78"}],"identifier":"id","label":"运行强化学习","html_id":"id-3","implicit":true,"key":"rKHAHNFe6N"},{"type":"paragraph","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"text","value":"设置和运行强化学习模型与运行回归器或分类器相同。命令行上必须定义同样的两个参数：","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"bs59sz96yY"},{"type":"inlineCode","value":"--freqaimodel","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"IEc2DJwM7e"},{"type":"text","value":" 和 ","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"whpRwDX9eg"},{"type":"inlineCode","value":"--strategy","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"Zx9ml7viCS"},{"type":"text","value":"：","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"aQzpK28HNx"}],"key":"hYsbbi3buo"},{"type":"code","lang":"bash","value":"freqtrade trade --freqaimodel ReinforcementLearner --strategy MyRLStrategy --config config.json","position":{"start":{"line":43,"column":1},"end":{"line":45,"column":1}},"key":"j4Me0co7f7"},{"type":"paragraph","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"其中 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"givoYtAjbW"},{"type":"inlineCode","value":"ReinforcementLearner","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"G0rtgu1mRG"},{"type":"text","value":" 将使用 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"zIk7nfEq7y"},{"type":"inlineCode","value":"freqai/prediction_models/ReinforcementLearner","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"doDWkrL8Jw"},{"type":"text","value":" 中的模板类（或 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"D8Kpedzwac"},{"type":"inlineCode","value":"user_data/freqaimodels","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"RWoFCgjWRb"},{"type":"text","value":" 下的自定义类）。策略部分则与典型回归器一样，采用","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"IHjcJUouAA"},{"type":"link","url":"/freqai-feature-engineering","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"特征工程","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"URxC6rB22i"}],"urlSource":"freqai-feature-engineering.md","dataUrl":"/freqai-feature-engineering.json","internal":true,"protocol":"file","key":"mKg9lYQrDp"},{"type":"text","value":"的 ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"TkAozMcmmv"},{"type":"inlineCode","value":"feature_engineering_*","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"Gs990y3YMH"},{"type":"text","value":"。不同之处在于目标的创建，强化学习不需要目标标签。然而，FreqAI 要求在动作列中设置一个默认（中立）值：","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"RQgZuCpHWq"}],"key":"orlRToc397"},{"type":"code","lang":"python","value":"    def set_freqai_targets(self, dataframe, **kwargs) -> DataFrame:\n        \"\"\"\n        *仅适用于启用 FreqAI 的策略*\n        设置模型目标的必需函数。\n        所有目标必须以 `&` 开头，以便 FreqAI 内部识别。\n\n        更多特征工程细节见：\n\n        https://www.freqtrade.io/en/latest/freqai-feature-engineering\n\n        :param df: 将接收目标的策略数据框\n        使用示例：dataframe[\"&-target\"] = dataframe[\"close\"].shift(-1) / dataframe[\"close\"]\n        \"\"\"\n        # 对于 RL，无需设置直接目标。此处为占位（中立），直到智能体发送动作。\n        dataframe[\"&-action\"] = 0\n        return dataframe","position":{"start":{"line":49,"column":1},"end":{"line":66,"column":1}},"key":"uSnqkCrLlR"},{"type":"paragraph","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"children":[{"type":"text","value":"大部分函数与典型回归器相同，但下方函数展示了策略如何将原始价格数据传递给智能体，以便其在训练环境中访问原始 OHLCV：","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"lMbfBO8vJX"}],"key":"vuYWxBlwf6"},{"type":"code","lang":"python","value":"    def feature_engineering_standard(self, dataframe: DataFrame, **kwargs) -> DataFrame:\n        # 以下特征对 RL 模型是必要的\n        dataframe[f\"%-raw_close\"] = dataframe[\"close\"]\n        dataframe[f\"%-raw_open\"] = dataframe[\"open\"]\n        dataframe[f\"%-raw_high\"] = dataframe[\"high\"]\n        dataframe[f\"%-raw_low\"] = dataframe[\"low\"]\n    return dataframe","position":{"start":{"line":70,"column":1},"end":{"line":78,"column":1}},"key":"dYAzh4MlXL"},{"type":"paragraph","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"children":[{"type":"text","value":"最后，没有显式的\"标签\"需要设置——而是需要分配 ","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"CKqbk1QIhd"},{"type":"inlineCode","value":"&-action","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"xY74KKJNQW"},{"type":"text","value":" 列，该列在 ","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"HQfkGU94Yk"},{"type":"inlineCode","value":"populate_entry/exit_trends()","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"Rcssva8pc8"},{"type":"text","value":" 中被访问时包含智能体的动作。在本例中，中立动作为 0。此值应与所用环境一致。FreqAI 提供的两个环境均以 0 作为中立动作。","position":{"start":{"line":80,"column":1},"end":{"line":80,"column":1}},"key":"vmZ6CbrWAZ"}],"key":"tPyqRW9YxX"},{"type":"paragraph","position":{"start":{"line":82,"column":1},"end":{"line":82,"column":1}},"children":[{"type":"text","value":"用户会很快意识到无需设置标签，智能体会\"自主\"做出进出场决策。这使得策略构建变得相当简单。进出场信号由智能体以整数形式给出，直接用于策略中的进出场判断：","position":{"start":{"line":82,"column":1},"end":{"line":82,"column":1}},"key":"xuyrgdZhn0"}],"key":"iQcdagNakQ"},{"type":"code","lang":"python","value":"    def populate_entry_trend(self, df: DataFrame, metadata: dict) -> DataFrame:\n\n        enter_long_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 1]\n\n        if enter_long_conditions:\n            df.loc[\n                reduce(lambda x, y: x & y, enter_long_conditions), [\"enter_long\", \"enter_tag\"]\n            ] = (1, \"long\")\n\n        enter_short_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 3]\n\n        if enter_short_conditions:\n            df.loc[\n                reduce(lambda x, y: x & y, enter_short_conditions), [\"enter_short\", \"enter_tag\"]\n            ] = (1, \"short\")\n\n        return df\n\n    def populate_exit_trend(self, df: DataFrame, metadata: dict) -> DataFrame:\n        exit_long_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 2]\n        if exit_long_conditions:\n            df.loc[reduce(lambda x, y: x & y, exit_long_conditions), \"exit_long\"] = 1\n\n        exit_short_conditions = [df[\"do_predict\"] == 1, df[\"&-action\"] == 4]\n        if exit_short_conditions:\n            df.loc[reduce(lambda x, y: x & y, exit_short_conditions), \"exit_short\"] = 1\n\n        return df","position":{"start":{"line":84,"column":1},"end":{"line":113,"column":1}},"key":"PVhdEY1RN6"},{"type":"paragraph","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"children":[{"type":"text","value":"需要注意的是，","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"D9aBtRkdgQ"},{"type":"inlineCode","value":"&-action","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"rdAZ07SPKs"},{"type":"text","value":" 取决于所选环境。上述示例展示了 5 个动作，其中 0 为中立，1 为多头开仓，2 为多头平仓，3 为空头开仓，4 为空头平仓。","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"kzUIWYkzJ8"}],"key":"j9rV0pjEuu"},{"type":"heading","depth":3,"position":{"start":{"line":117,"column":1},"end":{"line":117,"column":1}},"children":[{"type":"text","value":"配置强化学习器","position":{"start":{"line":117,"column":1},"end":{"line":117,"column":1}},"key":"XeoSUHMwPF"}],"identifier":"id","label":"配置强化学习器","html_id":"id-4","implicit":true,"key":"q8lCq52vzv"},{"type":"paragraph","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"children":[{"type":"text","value":"要配置 ","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"A87RDvLQ1F"},{"type":"inlineCode","value":"Reinforcement Learner","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"oRuadlp7Gp"},{"type":"text","value":"，需在 ","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"DtFQ0Bnjy3"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"pV995ch9Zq"},{"type":"text","value":" 配置中包含如下字典：","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"e3DdKcMHzX"}],"key":"SyfRrgSa9Y"},{"type":"code","lang":"json","value":"        \"rl_config\": {\n            \"train_cycles\": 25,\n            \"add_state_info\": true,\n            \"max_trade_duration_candles\": 300,\n            \"max_training_drawdown_pct\": 0.02,\n            \"cpu_count\": 8,\n            \"model_type\": \"PPO\",\n            \"policy_type\": \"MlpPolicy\",\n            \"model_reward_parameters\": {\n                \"rr\": 1,\n                \"profit_aim\": 0.025\n            }\n        }","position":{"start":{"line":121,"column":1},"end":{"line":135,"column":1}},"key":"MlbnSlq90v"},{"type":"paragraph","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"参数详情见","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"PE8nqB8BKG"},{"type":"link","url":"/freqai-parameter-table","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"参数表","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"kxeqqmCGSu"}],"urlSource":"freqai-parameter-table.md","dataUrl":"/freqai-parameter-table.json","internal":true,"protocol":"file","key":"dkuckGKDsk"},{"type":"text","value":"。一般来说，","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"gh58af8M4f"},{"type":"inlineCode","value":"train_cycles","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"XfoLpLrSjT"},{"type":"text","value":" 决定智能体在其人工环境中遍历蜡烛数据以训练模型权重的次数。","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"HJmgpiF8RO"},{"type":"inlineCode","value":"model_type","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"GDcyx0y0DU"},{"type":"text","value":" 是一个字符串，选择 ","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"RNJhufMJys"},{"type":"link","url":"https://stable-baselines3.readthedocs.io/en/master/","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"children":[{"type":"text","value":"stable_baselines","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"Ukn6a27pVr"}],"urlSource":"https://stable-baselines3.readthedocs.io/en/master/","key":"FILITo2hdB"},{"type":"text","value":"（外部链接）中可用的模型之一。","position":{"start":{"line":137,"column":1},"end":{"line":137,"column":1}},"key":"l5GPk3lmWQ"}],"key":"VEcXB8Eiqa"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"ug2tUdw1x0"}],"key":"WlAlYdIycJ"},{"type":"paragraph","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"children":[{"type":"text","value":"如果你想尝试 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"kfdoCFYXlR"},{"type":"inlineCode","value":"continual_learning","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"C8pdfw7xH6"},{"type":"text","value":"，应在主 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"zLNVGWMzz1"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"yEfI2LZ5TC"},{"type":"text","value":" 配置字典中将其设为 ","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"MALhidWJof"},{"type":"inlineCode","value":"true","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"mXYzJSTF3c"},{"type":"text","value":"。这会让强化学习库在每次再训练时，从前一模型的最终状态继续训练新模型，而不是每次都从头训练。","position":{"start":{"line":140,"column":1},"end":{"line":140,"column":1}},"key":"HS0r1ggU4H"}],"key":"mHa7h4IrxK"}],"key":"g3m1vZ47VK"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"MYa9UClHkL"}],"key":"fOpx7mvzjw"},{"type":"paragraph","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"children":[{"type":"text","value":"请记住，通用的 ","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"l3biCC7qyW"},{"type":"inlineCode","value":"model_training_parameters","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"eV7t0pEZsp"},{"type":"text","value":" 字典应包含特定 ","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"QUSmJLrkYB"},{"type":"inlineCode","value":"model_type","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"rYWVaimoVX"},{"type":"text","value":" 的所有模型超参数。例如，","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"aeF5zEeTSq"},{"type":"inlineCode","value":"PPO","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"YkjnBcma28"},{"type":"text","value":" 参数见","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"R9M3drEU7o"},{"type":"link","url":"https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"Ln1UoGDGtr"}],"urlSource":"https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html","key":"p5HWz0yLhW"},{"type":"text","value":"。","position":{"start":{"line":144,"column":1},"end":{"line":144,"column":1}},"key":"Zw1rJmK3JN"}],"key":"uHJp8Iiitu"}],"key":"wUXaENqMsv"},{"type":"heading","depth":3,"position":{"start":{"line":147,"column":1},"end":{"line":147,"column":1}},"children":[{"type":"text","value":"创建自定义奖励函数","position":{"start":{"line":147,"column":1},"end":{"line":147,"column":1}},"key":"IUn7QBEeeT"}],"identifier":"id","label":"创建自定义奖励函数","html_id":"id-5","implicit":true,"key":"J5u04zZ4oQ"},{"type":"admonition","kind":"danger","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"不适用于生产环境","position":{"start":{"line":149,"column":1},"end":{"line":149,"column":1}},"key":"F50QyKQZqW"}],"key":"NsHCfIr6lE"},{"type":"paragraph","position":{"start":{"line":150,"column":1},"end":{"line":150,"column":1}},"children":[{"type":"text","value":"警告！","position":{"start":{"line":150,"column":1},"end":{"line":150,"column":1}},"key":"AzvFhkD8pV"}],"key":"uqUbOEoQCP"},{"type":"paragraph","position":{"start":{"line":152,"column":1},"end":{"line":152,"column":1}},"children":[{"type":"text","value":"Freqtrade 源码中提供的奖励函数仅用于展示功能，旨在展示/测试尽可能多的环境控制特性，并在小型计算机上快速运行。它是基准，不适用于生产环境。请注意，你需要自己创建 custom_reward() 函数，或使用其他用户在 Freqtrade 源码之外构建的模板。","position":{"start":{"line":152,"column":1},"end":{"line":152,"column":1}},"key":"AiEi8PrNsN"}],"key":"XMSEQIydqe"}],"key":"UwFZzImbba"},{"type":"paragraph","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"当你开始修改策略和预测模型时，会很快发现强化学习器与回归器/分类器有一些重要区别。首先，策略不设置目标值（无标签！）。相反，你需要在 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"JOBc8Hwgxw"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"LfwiRafHHA"},{"type":"text","value":" 类中设置 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"s8HQe9h9TX"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"Xl9zkLQneF"},{"type":"text","value":" 函数（见下文）。","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"f0N8TBLHl9"},{"type":"inlineCode","value":"prediction_models/ReinforcementLearner.py","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"Ftb3CubmpD"},{"type":"text","value":" 中提供了一个默认的 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"sYZDIs1xRL"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"qfJDIPXZKU"},{"type":"text","value":"，用于演示奖励构建的必要模块，但","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"MNYBpVoAF8"},{"type":"emphasis","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"不适用于生产","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"nsQboiCFgx"}],"key":"wz66I4G6U2"},{"type":"text","value":"。用户","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"RWSCOhUno0"},{"type":"emphasis","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"children":[{"type":"text","value":"必须","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"nJS9oZSTU2"}],"key":"DPf8KBfq4P"},{"type":"text","value":"创建自己的自定义强化学习模型类，或使用 Freqtrade 源码之外的预构建模型并保存到 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"lE9gnUstOd"},{"type":"inlineCode","value":"user_data/freqaimodels","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"AQ7ICLwyVv"},{"type":"text","value":"。在 ","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"nrxmpNHHME"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"LQRmBDydgY"},{"type":"text","value":" 中可以表达你对市场的创造性理论。例如，你可以在智能体获利时奖励它，亏损时惩罚它，或奖励其开仓、惩罚其持仓过久。下方展示了这些奖励的计算方式：","position":{"start":{"line":155,"column":1},"end":{"line":155,"column":1}},"key":"ryqf5V2f8V"}],"key":"TZ2h52AaMp"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"提示","position":{"start":{"line":157,"column":1},"end":{"line":157,"column":1}},"key":"oFsOTvuDjc"}],"key":"ZB4RTN5d1e"},{"type":"paragraph","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"children":[{"type":"text","value":"最好的奖励函数是连续可微且缩放良好的。换句话说，对罕见事件施加一次性大负惩罚不是好主意，神经网络无法学会这种函数。更好的做法是对常见事件施加小负惩罚，这有助于智能体更快学习。你还可以通过让奖励/惩罚随某些线性/指数函数按严重程度缩放来提升连续性。例如，随着持仓时间增加，逐步增加惩罚，这比在某一时刻一次性施加大惩罚更好。","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"key":"swo0GrfXg8"}],"key":"tLV6cVUHBd"}],"key":"YkQ2n8YiQU"},{"type":"code","lang":"python","value":"from freqtrade.freqai.prediction_models.ReinforcementLearner import ReinforcementLearner\nfrom freqtrade.freqai.RL.Base5ActionRLEnv import Actions, Base5ActionRLEnv, Positions\n\n\nclass MyCoolRLModel(ReinforcementLearner):\n    \"\"\"\n    用户自定义 RL 预测模型。\n\n    将此文件保存到 `freqtrade/user_data/freqaimodels`\n\n    然后用如下命令调用：\n\n    freqtrade trade --freqaimodel MyCoolRLModel --config config.json --strategy SomeCoolStrat\n\n    用户可重写 `IFreqaiModel` 继承树中的任意函数。对 RL 来说，最重要的是在此重写 `MyRLEnv`（见下文），以定义自定义 `calculate_reward()`，或重写环境的其他部分。\n\n    此类还允许用户重写 IFreqaiModel 树的其他部分。例如，可以重写 `def fit()`、`def train()` 或 `def predict()` 以精细控制这些过程。\n\n    另一个常见重写是 `def data_cleaning_predict()`，用于精细控制数据处理管道。\n    \"\"\"\n    class MyRLEnv(Base5ActionRLEnv):\n        \"\"\"\n        用户自定义环境。此类继承自 BaseEnvironment 和 gym.Env。\n        用户可重写父类的任意函数。以下为自定义 `calculate_reward()` 的示例。\n\n        警告！\n        此函数仅用于展示功能，旨在展示尽可能多的环境控制特性，并在小型计算机上快速运行。它是基准，不适用于生产环境。\n        \"\"\"\n        def calculate_reward(self, action: int) -> float:\n            # 首先，若动作无效则惩罚\n            if not self._is_valid(action):\n                return -2\n            pnl = self.get_unrealized_profit()\n\n            factor = 100\n\n            pair = self.pair.replace(':', '')\n\n            # 可使用 dataframe 中的特征值\n            # 假设策略中已生成移位的 RSI 指标。\n            rsi_now = self.raw_features[f\"%-rsi-period_10_shift-1_{pair}_\"\n                            f\"{self.config['timeframe']}\"]\\\n                            .iloc[self._current_tick]\n\n            # 奖励智能体开仓\n            if (action in (Actions.Long_enter.value, Actions.Short_enter.value)\n                    and self._position == Positions.Neutral):\n                if rsi_now < 40:\n                    factor = 40 / rsi_now\n                else:\n                    factor = 1\n                return 25 * factor\n\n            # 惩罚智能体未开仓\n            if action == Actions.Neutral.value and self._position == Positions.Neutral:\n                return -1\n            max_trade_duration = self.rl_config.get('max_trade_duration_candles', 300)\n            trade_duration = self._current_tick - self._last_trade_tick\n            if trade_duration <= max_trade_duration:\n                factor *= 1.5\n            elif trade_duration > max_trade_duration:\n                factor *= 0.5\n            # 惩罚持仓不动\n            if self._position in (Positions.Short, Positions.Long) and \\\n            action == Actions.Neutral.value:\n                return -1 * trade_duration / max_trade_duration\n            # 多头平仓\n            if action == Actions.Long_exit.value and self._position == Positions.Long:\n                if pnl > self.profit_aim * self.rr:\n                    factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n                return float(pnl * factor)\n            # 空头平仓\n            if action == Actions.Short_exit.value and self._position == Positions.Short:\n                if pnl > self.profit_aim * self.rr:\n                    factor *= self.rl_config['model_reward_parameters'].get('win_reward_factor', 2)\n                return float(pnl * factor)\n            return 0.","position":{"start":{"line":161,"column":1},"end":{"line":239,"column":1}},"key":"BoA0y3B6Ep"},{"type":"heading","depth":3,"position":{"start":{"line":241,"column":1},"end":{"line":241,"column":1}},"children":[{"type":"text","value":"使用 Tensorboard","position":{"start":{"line":241,"column":1},"end":{"line":241,"column":1}},"key":"jFaMqm1qnR"}],"identifier":"id-tensorboard","label":"使用 Tensorboard","html_id":"id-tensorboard","implicit":true,"key":"NFcM25Pm9s"},{"type":"paragraph","position":{"start":{"line":243,"column":1},"end":{"line":243,"column":1}},"children":[{"type":"text","value":"强化学习模型受益于训练指标的跟踪。FreqAI 集成了 Tensorboard，允许用户跨所有币种和所有再训练过程跟踪训练和评估表现。可通过以下命令激活 Tensorboard：","position":{"start":{"line":243,"column":1},"end":{"line":243,"column":1}},"key":"lTANqifiiX"}],"key":"NOpuowAbWh"},{"type":"code","lang":"bash","value":"tensorboard --logdir user_data/models/unique-id","position":{"start":{"line":245,"column":1},"end":{"line":247,"column":1}},"key":"JbDM7xPrJZ"},{"type":"paragraph","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"children":[{"type":"text","value":"其中 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"ngv2Uqp53D"},{"type":"inlineCode","value":"unique-id","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"IFtCVQqSCX"},{"type":"text","value":" 是 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"rct1LuaX11"},{"type":"inlineCode","value":"freqai","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"trRS3e4Oes"},{"type":"text","value":" 配置文件中设置的 ","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"m77fFQ9ta1"},{"type":"inlineCode","value":"identifier","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"NHXDDQK7H1"},{"type":"text","value":"。该命令需在单独的 shell 中运行，浏览器访问 127.0.0.1:6006（6006 为 Tensorboard 默认端口）查看输出。","position":{"start":{"line":249,"column":1},"end":{"line":249,"column":1}},"key":"KqAOGloUMJ"}],"key":"jOFV46ESTR"},{"type":"image","url":"/build/tensorboard-27f8850881840b007f5a56a202abb949.jpg","alt":"tensorboard","position":{"start":{"line":251,"column":1},"end":{"line":251,"column":1}},"key":"qT6qPQun4h","urlSource":"assets/tensorboard.jpg","urlOptimized":"/build/tensorboard-27f8850881840b007f5a56a202abb949.webp"},{"type":"heading","depth":3,"position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"children":[{"type":"text","value":"自定义日志","position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"key":"WRDzE7ts8q"}],"identifier":"id","label":"自定义日志","html_id":"id-6","implicit":true,"key":"pn5oCmdg0l"},{"type":"paragraph","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"children":[{"type":"text","value":"FreqAI 还内置了一个名为 ","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"aIZzvM0euf"},{"type":"inlineCode","value":"self.tensorboard_log","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"bH5df2cKju"},{"type":"text","value":" 的分集摘要日志器，用于将自定义信息添加到 Tensorboard 日志。默认情况下，该函数在环境内每步调用一次以记录智能体动作。单集内所有步的值在每集结束时汇总报告，然后所有指标重置为 0，为下一个 episode 做准备。","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"fwbhfnD16s"}],"key":"fL9pUIG7iL"},{"type":"paragraph","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"children":[{"type":"inlineCode","value":"self.tensorboard_log","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"LbFFfF0Pch"},{"type":"text","value":" 也可在环境内任意位置使用，例如可在 ","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"WHJMnduavU"},{"type":"inlineCode","value":"calculate_reward","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"ia6OOkp6m1"},{"type":"text","value":" 函数中添加，以收集奖励各部分被调用的频率：","position":{"start":{"line":257,"column":1},"end":{"line":257,"column":1}},"key":"VdZnQaD39R"}],"key":"OIxRmLAB9v"},{"type":"code","lang":"python","value":"    class MyRLEnv(Base5ActionRLEnv):\n        \"\"\"\n        用户自定义环境。此类继承自 BaseEnvironment 和 gym.Env。\n        用户可重写父类的任意函数。以下为自定义 `calculate_reward()` 的示例。\n        \"\"\"\n        def calculate_reward(self, action: int) -> float:\n            if not self._is_valid(action):\n                self.tensorboard_log(\"invalid\")\n                return -2\n","position":{"start":{"line":259,"column":1},"end":{"line":270,"column":1}},"key":"trq2uUxdAV"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"WlMQgS2rS0"}],"key":"hJBkPqve4o"},{"type":"paragraph","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"children":[{"type":"inlineCode","value":"self.tensorboard_log()","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"SxZmWoWPMc"},{"type":"text","value":" 仅用于跟踪计数对象（如事件、动作）。如需记录浮点数，可作为第二参数传入，如 ","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"vRqrfGLBK7"},{"type":"inlineCode","value":"self.tensorboard_log(\"float_metric1\", 0.23)","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"tead8bnOqs"},{"type":"text","value":"。此时指标值不会累加。","position":{"start":{"line":273,"column":1},"end":{"line":273,"column":1}},"key":"PQDLyFhto1"}],"key":"m6Eo2f32jG"}],"key":"moL56evkwK"},{"type":"heading","depth":3,"position":{"start":{"line":276,"column":1},"end":{"line":276,"column":1}},"children":[{"type":"text","value":"选择基础环境","position":{"start":{"line":276,"column":1},"end":{"line":276,"column":1}},"key":"BCE4Y06QVy"}],"identifier":"id","label":"选择基础环境","html_id":"id-7","implicit":true,"key":"fUagGkB8mZ"},{"type":"paragraph","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"children":[{"type":"text","value":"FreqAI 提供三种基础环境：","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"nu3UzTNN7r"},{"type":"inlineCode","value":"Base3ActionRLEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"w70KM7Y5CI"},{"type":"text","value":"、","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"OjVGCNT6LK"},{"type":"inlineCode","value":"Base4ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"VhMpyRQ2wy"},{"type":"text","value":" 和 ","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"xXqYFL7UeO"},{"type":"inlineCode","value":"Base5ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"joeIEM0WnB"},{"type":"text","value":"。顾名思义，这些环境分别适用于可选择 3、4、5 种动作的智能体。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"j7fdGuLHpX"},{"type":"inlineCode","value":"Base3ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"bda8oEqUeW"},{"type":"text","value":" 最简单，智能体可选择持有、多头或空头。该环境也可用于仅做多的机器人（自动跟随策略的 ","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"xh4b8WPt0y"},{"type":"inlineCode","value":"can_short","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"EWE0PdaJqt"},{"type":"text","value":" 标志），其中多头为开仓，空头为平仓。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"b5t5zP5Xzb"},{"type":"inlineCode","value":"Base4ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"vShXOhgaxh"},{"type":"text","value":" 中，智能体可多头开仓、空头开仓、中立持有或平仓。","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"guZqMLdg17"},{"type":"inlineCode","value":"Base5ActionEnvironment","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"Nqwqm5s83T"},{"type":"text","value":" 则与 Base4 类似，但将平仓动作区分为多头平仓和空头平仓。环境选择的主要影响包括：","position":{"start":{"line":278,"column":1},"end":{"line":278,"column":1}},"key":"BN7kC8X8zD"}],"key":"nMXRMAQeMP"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":280,"column":1},"end":{"line":282,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"children":[{"type":"inlineCode","value":"calculate_reward","position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"key":"HgiDOmOl12"},{"type":"text","value":" 中可用的动作","position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"key":"YPQj86Qjrf"}],"key":"S6qI8DQbPl"},{"type":"listItem","spread":true,"position":{"start":{"line":281,"column":1},"end":{"line":282,"column":1}},"children":[{"type":"text","value":"用户策略中消费的动作","position":{"start":{"line":281,"column":1},"end":{"line":281,"column":1}},"key":"Ksyg9PjGZl"}],"key":"iRij9ykEDs"}],"key":"IhgMq3F4LA"},{"type":"paragraph","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"children":[{"type":"text","value":"FreqAI 提供的所有环境均继承自动作/持仓无关的 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"zvmK2o0g9A"},{"type":"inlineCode","value":"BaseEnvironment","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"GyCvPIyc7B"},{"type":"text","value":"，该对象包含所有共享逻辑。架构设计易于自定义。最简单的自定义是 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"vcNJg9Bp9B"},{"type":"inlineCode","value":"calculate_reward()","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"pYazjee50c"},{"type":"text","value":"（详见","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"JApl7LOOQx"},{"type":"link","url":"#creating-a-custom-reward-function","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"children":[{"type":"text","value":"此处","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"g5GSuG3Z4S"}],"urlSource":"#creating-a-custom-reward-function","key":"VlIkOZtERj"},{"type":"text","value":"）。当然，也可进一步扩展环境内的任意函数，只需在预测模型文件的 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"weAaCStG9w"},{"type":"inlineCode","value":"MyRLEnv","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"BQvvQs4wuc"},{"type":"text","value":" 中重写这些函数即可。更高级的自定义建议直接继承 ","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"cH4kxkTDWu"},{"type":"inlineCode","value":"BaseEnvironment","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"SEeIy0btsd"},{"type":"text","value":" 创建全新环境。","position":{"start":{"line":283,"column":1},"end":{"line":283,"column":1}},"key":"LOgQ7hfMD2"}],"key":"h3Xn7FpMIW"},{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"IU915jd11I"}],"key":"PEEUsc13Lz"},{"type":"paragraph","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"children":[{"type":"text","value":"仅 ","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"lOatfnHVy9"},{"type":"inlineCode","value":"Base3ActionRLEnv","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"UYFn8OHmZd"},{"type":"text","value":" 支持仅做多训练/交易（将用户策略属性 ","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"mysVAet7UW"},{"type":"inlineCode","value":"can_short = False","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"sY8DqLwihp"},{"type":"text","value":"）。","position":{"start":{"line":286,"column":1},"end":{"line":286,"column":1}},"key":"Ood1PBnZMQ"}],"key":"KqVa5DDruj"}],"key":"mPJfXY6VRw"}],"key":"ty8O0JomeN"}],"key":"ykdi8qeNRS"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"FreqAI 运行指南","short_title":"FreqAI 运行","url":"/freqai-running","group":"FreqAI"},"next":{"title":"FreqAI 开发者指南","short_title":"FreqAI 开发","url":"/freqai-developers","group":"FreqAI"}}},"domain":"http://localhost:3002"}